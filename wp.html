<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:DengXian;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:"Segoe UI";
	panose-1:2 11 6 4 2 2 2 2 2 4;}
@font-face
	{font-family:"\@DengXian";
	panose-1:2 1 6 0 3 1 1 1 1 1;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:0cm;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Times New Roman",serif;}
h2
	{mso-style-link:"Heading 2 Char";
	margin-right:0cm;
	margin-left:0cm;
	font-size:18.0pt;
	font-family:"Times New Roman",serif;
	font-weight:bold;}
h3
	{mso-style-link:"Heading 3 Char";
	margin-top:2.0pt;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:0cm;
	line-height:107%;
	page-break-after:avoid;
	font-size:12.0pt;
	font-family:"Calibri Light",sans-serif;
	color:#1F4D78;
	font-weight:normal;}
h4
	{mso-style-link:"Heading 4 Char";
	margin-top:2.0pt;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:0cm;
	line-height:107%;
	page-break-after:avoid;
	font-size:11.0pt;
	font-family:"Calibri Light",sans-serif;
	color:#2E74B5;
	font-weight:normal;
	font-style:italic;}
p.MsoFootnoteText, li.MsoFootnoteText, div.MsoFootnoteText
	{mso-style-link:"Footnote Text Char";
	margin:0cm;
	font-size:10.0pt;
	font-family:"Times New Roman",serif;}
p.MsoCommentText, li.MsoCommentText, div.MsoCommentText
	{mso-style-link:"Comment Text Char";
	margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:0cm;
	font-size:10.0pt;
	font-family:"Times New Roman",serif;}
p.MsoHeader, li.MsoHeader, div.MsoHeader
	{mso-style-link:"Header Char";
	margin:0cm;
	font-size:11.0pt;
	font-family:"Times New Roman",serif;}
p.MsoFooter, li.MsoFooter, div.MsoFooter
	{mso-style-link:"Footer Char";
	margin:0cm;
	font-size:11.0pt;
	font-family:"Times New Roman",serif;}
span.MsoFootnoteReference
	{vertical-align:super;}
span.MsoEndnoteReference
	{vertical-align:super;}
p.MsoEndnoteText, li.MsoEndnoteText, div.MsoEndnoteText
	{mso-style-link:"Endnote Text Char";
	margin:0cm;
	font-size:10.0pt;
	font-family:"Times New Roman",serif;}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{color:#954F72;
	text-decoration:underline;}
p.MsoPlainText, li.MsoPlainText, div.MsoPlainText
	{mso-style-link:"Plain Text Char";
	margin:0cm;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p
	{margin-right:0cm;
	margin-left:0cm;
	font-size:12.0pt;
	font-family:"Times New Roman",serif;}
p.MsoCommentSubject, li.MsoCommentSubject, div.MsoCommentSubject
	{mso-style-link:"Comment Subject Char";
	margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:0cm;
	font-size:10.0pt;
	font-family:"Times New Roman",serif;
	font-weight:bold;}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-link:"Balloon Text Char";
	margin:0cm;
	font-size:9.0pt;
	font-family:"Segoe UI",sans-serif;}
p.MsoRMPane, li.MsoRMPane, div.MsoRMPane
	{margin:0cm;
	font-size:11.0pt;
	font-family:"Times New Roman",serif;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:36.0pt;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Times New Roman",serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:36.0pt;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Times New Roman",serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:36.0pt;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Times New Roman",serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:8.0pt;
	margin-left:36.0pt;
	line-height:107%;
	font-size:11.0pt;
	font-family:"Times New Roman",serif;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-link:"Heading 2";
	font-family:"Times New Roman",serif;
	font-weight:bold;}
span.mw-headline
	{mso-style-name:mw-headline;}
span.mw-editsection
	{mso-style-name:mw-editsection;}
span.mw-editsection-bracket
	{mso-style-name:mw-editsection-bracket;}
span.Heading3Char
	{mso-style-name:"Heading 3 Char";
	mso-style-link:"Heading 3";
	font-family:"Calibri Light",sans-serif;
	color:#1F4D78;}
span.FootnoteTextChar
	{mso-style-name:"Footnote Text Char";
	mso-style-link:"Footnote Text";}
span.weblist-label
	{mso-style-name:web_list-label;}
span.CommentTextChar
	{mso-style-name:"Comment Text Char";
	mso-style-link:"Comment Text";}
span.CommentSubjectChar
	{mso-style-name:"Comment Subject Char";
	mso-style-link:"Comment Subject";
	font-weight:bold;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-link:"Balloon Text";
	font-family:"Segoe UI",sans-serif;}
span.HeaderChar
	{mso-style-name:"Header Char";
	mso-style-link:Header;}
span.FooterChar
	{mso-style-name:"Footer Char";
	mso-style-link:Footer;}
span.EndnoteTextChar
	{mso-style-name:"Endnote Text Char";
	mso-style-link:"Endnote Text";}
span.PlainTextChar
	{mso-style-name:"Plain Text Char";
	mso-style-link:"Plain Text";
	font-family:"Calibri",sans-serif;}
p.xmsonormal, li.xmsonormal, div.xmsonormal
	{mso-style-name:x_msonormal;
	margin:0cm;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.ji, li.ji, div.ji
	{mso-style-name:ji;
	margin-right:0cm;
	margin-left:0cm;
	font-size:12.0pt;
	font-family:"Times New Roman",serif;}
span.Heading4Char
	{mso-style-name:"Heading 4 Char";
	mso-style-link:"Heading 4";
	font-family:"Calibri Light",sans-serif;
	color:#2E74B5;
	font-style:italic;}
.MsoChpDefault
	{font-size:11.0pt;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:107%;}
 /* Page Definitions */
 @page WordSection1
	{size:612.0pt 792.0pt;
	margin:72.0pt 72.0pt 72.0pt 72.0pt;}
div.WordSection1
	{page:WordSection1;}
 /* List Definitions */
 ol
	{margin-bottom:0cm;}
ul
	{margin-bottom:0cm;}
-->
</style>

</head>

<body lang=en-DK link=blue vlink="#954F72" style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US
style='color:black;background:white'>&nbsp;A Framework for Understanding
AI-Induced Field Change: How AI Technologies are Legitimized and
Institutionalized</span></b></p>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US>(Working
Paper – Unfinished Draft – Do Not Cite)</span></b></p>

<p class=MsoNormal align=center style='text-align:center'><span lang=EN-US>&nbsp;</span></p>

<p class=MsoNormal align=center style='text-align:center'><span lang=EN-US>Benjamin
Cedric Larsen</span></p>

<p class=MsoNormal align=center style='text-align:center'><span lang=EN-US>Ph.D.
Fellow - Copenhagen Business School </span></p>

<p class=MsoNormal><b><u><span lang=EN-US><span style='text-decoration:none'>&nbsp;</span></span></u></b></p>

<p class=MsoNormal><b><u><span lang=EN-US><span style='text-decoration:none'>&nbsp;</span></span></u></b></p>

<p class=MsoNormal align=center style='text-align:center'><b><span lang=EN-US>Abstract</span></b></p>

<p class=MsoNormal><span lang=EN-US>Artificial intelligence (AI) systems
operate in increasingly diverse areas, from healthcare to facial recognition,
the stock market, autonomous vehicles, and so on. While the underlying digital
infrastructure of AI systems is developing rapidly, each area of implementation
is subject to different degrees and processes of legitimization. By combining elements
from institutional theory and information systems-theory, this paper presents a
conceptual framework to analyze and understand AI-induced field-change. The
framework is illustrated through application to the development and adoption of
facial recognition technologies in the United States. The introduction of novel
AI-agents into new or existing fields creates a dynamic in which algorithms (re)shape
organizations and institutions while existing institutional infrastructures
determine the scope and speed at which organizational change is allowed to
occur. Where institutional infrastructure and governance arrangements, such as
standards, rules, and regulations, are unelaborate, the field can move fast but
is also more likely to be contested. </span><span lang=EN-US>The institutional
infrastructure surrounding AI-induced fields is generally little elaborated,
which could be an obstacle to the broader institutionalization of AI-systems
going forward.</span></p>

<p class=MsoNormal><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN-US>&nbsp;</span></b></p>

<p class=MsoListParagraph style='text-indent:-18.0pt'><b><span lang=EN-US>1.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp; </span></span></b><b><span
lang=EN-US>Introduction </span></b></p>

<p class=MsoNormal><span lang=EN-US>In recent years, the scope of information
technology that complements or augments human actions has expanded rapidly. The
logics embedded in AI-systems already operate in diverse areas, such as the
stock market </span><span
lang=EN-US>(Mackenzie, 2006)</span><span lang=EN-US>, mortgage underwriting </span><span
lang=EN-US>(Markus, 2017)</span><span lang=EN-US>, autonomous vehicles </span><span
lang=EN-US>(Hengstler, Enkel, &amp; Duelli, 2016)</span><span lang=EN-US>, medical
services </span><span
lang=EN-US>(Davenport &amp; Kalakota, 2019), the judicial system </span><span
lang=EN-US>(Mckay, 2020)</span><span lang=EN-US>, and a range of other fields. The
action-potentials inherent in most AI systems imply a shift in agency, moving
from human actors to AI agents, which in turn has a significant impact on
shaping new practices (e.g., across healthcare, agriculture, autonomous
vehicles, etc.), and thereby new forms of organization. </span></p>

<p class=MsoNormal><span lang=EN-US>Novel AI systems and agents are embedded in
existing digital infrastructures and operate within an institutional framework
that enables or constrains various activities </span><span
lang=EN-US>(Baskerville, Myers, &amp; Yoo, 2019)</span><span lang=EN-US>. The
socio-economic embeddedness of AI systems means that some AI agents may affect
and alter existing social practices and ways of organization in swift and
transforming ways, while implementation may be subject to varying degrees of
legitimacy, depending on the field and area of implementation. </span></p>

<p class=MsoNormal><span lang=EN-US>During this process, human actions themselves
have become subject to informatization where behavior is tracked and data
points collected </span><span
lang=EN-US>(Kallinikos, 2011</span><span lang=EN-US>;</span><span
lang=EN-US> Zuboff, 1988</span><span lang=EN-US>,</span><span
lang=EN-US> 2019)</span><span lang=EN-US>. Data is derived from social networks
and online interactions, facial recognition technologies, driving behavior,
apps </span><span lang=EN-US>recording location data, and so on.</span><span
lang=EN-US> The wide range of AI implementations motivates the research
question of this paper, which seeks to understand how AI-induced fields are
subject to varying degrees of legitimacy as well as processes of
institutionalization.</span></p>

<p class=MsoNormal><span lang=EN-US>Views from institutional- and information
systems (IS) theory are combined in order to </span><span lang=EN-US>conceptualize
how AI fields operate at the meso-level in terms of gaining legitimacy, that
is, how AI diffusion is adopted and accepted, or rejected, under varying
socio-economic conditions. </span></p>

<p class=MsoNormal><span lang=EN-US>Elements from information systems theory
elaborate on the </span><span lang=EN-US>notion of digital infrastructure </span><span
lang=EN-US>(Constantinides, Henfridsson, &amp; Parker, 2018</span><span
lang=EN-US>; Henfridsson &amp; Bygstad, 2013; </span><span
lang=EN-US>Yoo, Henfridsson, &amp; Lyytinen, 2010</span><span lang=EN-US>),
which signifies a range of interconnected technologies (e.g., Internet,
Platforms, IoT) that contribute to realize the action potentials of novel AI
agents and associated processes of information collection. </span></p>

<p class=MsoNormal><span lang=EN-US>Institutional theory introduces the concept
of fields, which is applied in order to denote distinct areas of AI
implementation and organization by a diverse range of actors</span><span
lang=EN-US>. </span><span lang=EN-US>Elements from institutional theory, i.e.,
institutional work </span><span
lang=EN-US>(Lawrence &amp; Suddaby, 2006)</span><span lang=EN-US>, logics </span><span
lang=EN-US>(Thornton, Ocasio, &amp; Lounsbury, 2012</span><span lang=EN-US>; </span><span
lang=EN-US>Gawer &amp; Phillips, 2013)</span><span lang=EN-US>, and
infrastructure </span><span
lang=EN-US>(Hinings, Logue, &amp; Zietsma, 2017)</span><span lang=EN-US>, are
applied in order to conceptualize how processes of AI-induced digitization
affects the evolution and governance of organizations </span><span
lang=EN-US>(Powell, Oberg, Korff, Oelberger, &amp; Kloos, 2017)</span><span
lang=EN-US>. </span><span lang=EN-US>Theory surrounding institutional work is
applied in order to understand how actors accomplish the social construction of
logics (i.e., rules, scripts, schemas, and cultural accounts), which signifies
where human actors and AI agents may challenge existing organizational or
institutional practices and boundaries, which may result in difficulties
associated with legitimization. </span><span lang=EN-US>Adding the
institutional perspective is about how “digitally-enabled institutional
arrangements emerge and diffuse both through fields and organizations” </span><span
lang=EN-US>(Hinings, Gegenhuber, &amp; Greenwood, 2018: 53)</span><span
lang=EN-US>. The primary focus of the paper is placed on the interplay between
existing and new and emerging institutional arrangements, as well as the role
of AI in altering ways of organization. </span></p>

<p class=MsoNormal><span lang=EN-US>In combining views from institutional- and
information systems (IS) theory, the paper proposes a novel conceptual </span><span
lang=EN-US>framework for analyzing and understanding AI-induced field change.
The framework builds on </span><span
lang=EN-US>Zietsma et al.’s. (2017) concept of pathways of change, which outlines
how a field is likely to move between states from emerging/aligning to
fragmented, contested, and established, depending on the coherency in logics
and elaboration of institutional infrastructure. </span><span lang=EN-US>The
proposed framework adds the notion of digital infrastructure elaborated through
the constructs of </span><span lang=EN-US>technological maturity, data, and AI
autonomy, which enables an assessment of the impact of AI-systems on existing
forms of institutional infrastructure. </span><span lang=EN-US>Where</span><span
lang=EN-US> digital and institutional infrastructure is well-elaborated in
terms of organizational practices, rules, and processes, the field could be
considered established. If a field is emerging or aligning, on the other hand,
its digital and institutional infrastructure will be nascent and unelaborate. The
developed framework is illustrated through application to the field of facial
recognition technologies in the United States.</span></p>

<p class=MsoNormal><span lang=EN-US>The paper makes three contributions. First,
it presents a conceptual framework for analyzing AI-induced field change.
Second, through illustration of the framework, three common grounds for
contestation are found, which obstruct processes of legitimization associated
with AI-induced field change. These relate to: </span><span lang=EN-US>altered
power-dependencies between humans and machines; the importance of data as a
source of legitimacy, as well as; insufficient forms of governance, i.e., self-regulation,
which warrants a shift towards greater elaboration of official institutional
infrastructure surrounding AI applications.</span><span lang=EN-US> </span><span
lang=EN-US>At last, the paper discusses implications of AI-based
institutionalization going forward and highlights the need for more adaptive
organizations to emerge in order to keep pace with AI-systems’ transformative
impact on existing practices and ways of behavior.  </span></p>

<p class=MsoNormal><span lang=EN-US>The paper is structured as follows. Section
2 elaborates on institutional theory and the characteristics of digital
infrastructure. Section 3 presents a </span><span lang=EN-US>framework for
understanding AI-induced field change</span><span lang=EN-US>. Section 4 applies
the framework through illustration. Section 5 deliberates on pathways of change
referring to how AI-fields become institutionalized, and section 6 discusses obstacles
to legitimacy as well as paths forward in terms of governance. Section 7
concludes.</span></p>

<p class=MsoListParagraph style='text-indent:-18.0pt'><b><span lang=EN-US>2.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp; </span></span></b><b><span
lang=EN-US>Institutional Theory and AI-Agents</span></b></p>

<p class=MsoCommentText><span lang=EN-US style='font-size:11.0pt'>In
organization theory, the idea of institutional infrastructure reflects
understandings of the embeddedness of organizations within fields and the
structuration of fields that occurs through interactions and institutional
activity amongst actors </span><span
lang=EN-US style='font-size:11.0pt'>(Dacin, Ventresca, &amp; Beal, 1999)</span><span
lang=EN-US style='font-size:11.0pt'>. Over the last few decades, organizational
fields have become more dynamic, and boundaries between fields have become more
porous due to the introduction of new digital infrastructures, such as the
Internet </span><span
lang=EN-US style='font-size:11.0pt'>(Powell et al., 2017: 336)</span><span
lang=EN-US style='font-size:11.0pt'>. </span></p>

<p class=MsoNormal><span lang=EN-US>Early institutional theory developed the
notion that organizations come to resemble each other due to socio-cultural
pressures, which provide a source of legitimacy </span><span
lang=EN-US>(Meyer &amp; Rowan, 1977)</span><span lang=EN-US>. A central process
is that of isomorphism, demonstrating that organizations are likely to converge
through normative, mimetic, and coercive pressures </span><span
lang=EN-US>(DiMaggio &amp; Powell, 1983)</span><span lang=EN-US>. Mimetic
isomorphism holds that organizational legitimacy is achieved through copying
other organizations as well as their technologies and practices. Coercive
legitimacy refers to societal legitimacy, which often is achieved through
legislation, whereas normative legitimacy can be viewed as the appropriate
professional standards as well as social acceptance of new technologies. Socio-cultural
beliefs and practices thus play an important role in the adoption of new
technologies and innovations, as well as contingent processes of legitimization
and organizational change </span><span
lang=EN-US>(Hinings et al., 2018)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>Competing institutions may lie within
individual populations that inhabit a field, while fields may be contested by
multiple, and often competing, institutional logics </span><span
lang=EN-US>(Gawer &amp; Phillips, 2013; </span><span
lang=EN-US>Greenwood, Raynard, Kodeih, Micelotta, &amp; Lounsbury, 2011</span><span
lang=EN-US>; </span><span
lang=EN-US>Reay &amp; Hinings, 2005</span><span lang=EN-US>, </span><span
lang=EN-US>2009</span><span lang=EN-US>; </span><span
lang=EN-US>Scott, 2014)</span><span lang=EN-US>. Institutional logics describe
the </span><span lang=EN-US>“socially constructed, historical patterns of
material practices, assumptions, values, beliefs, and rules” of a field  </span><span
lang=EN-US>(Thornton &amp; Ocasio, 1999</span><span lang=EN-US>: 804). T</span><span
lang=EN-US>he institutional logics perspective deals with the
interrelationships among individuals, institutions, and organizations, i.e.,
the actors </span><span lang=EN-US>of a field.</span><span lang=EN-US> </span></p>

<p class=MsoNormal><span lang=EN-US>Institutional work, on the other hand,
emphasizes a </span><span lang=EN-US>conceptual shift towards individuals and
organization’s actions as “dependent on cognitive (rather than affective)
processes and structures and thus suggests an approach… that focuses on
understanding how actors accomplish the social construction of rules, scripts,
schemas, and cultural accounts” </span><span
lang=EN-US>(Lawrence &amp; Suddaby, 2006: 218)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>When the two approaches are held together, i.e.,
logics and interrelationships, and structures and practices, these can be
expressed as the institutional infrastructure of a field. </span><span
lang=EN-US>Institutional infrastructure is established through adjacent
activities such as certifying, assuring, and reporting against principles,
codes, and standards, as well as through the formation of new associations and
networks among organizations, including official rules and regulations </span><span
lang=EN-US>(Waddock, 2008)</span><span lang=EN-US>. Institutional
infrastructure can be clarified in terms of its degree of elaboration (high,
low), as well as coherency in logics (unitary, competing) (</span><span
lang=EN-US>Hinings et al., 2017)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>Novel AI agents operating in varying
systems also embody distinct logics and cognitive functions </span><span
lang=EN-US>(Floridi &amp; Sanders, 2004)</span><span lang=EN-US>. While these
functions are defined by human actors (e.g., engineers in a company), AI-agents
remain subject to different degrees of autonomy, i.e., they are to some extent able
to act independently based on intrinsic flows of information. This implies that
AI agents have the autonomy to act on (e.g., judicial evidence, road
conditions, etc.), as well as interact with (e.g., speech recognition,
chatbots) their environments. This new form of artificial agency confounds the
paradox of embedded agency, i.e., how actors are able to change institutions
when their actions are conditioned by those same institutions </span><span
lang=EN-US>(Holm, 1995)</span><span lang=EN-US>, by the implication of an AI’s
ability to shape human behavior as well as ways of organization – sometimes simultaneously.
In other words, algorithms can affect how we conceptualize the world while
modifying socio-political forms of organization </span><span
lang=EN-US>(Floridi, 2014)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>Algorithms can be seen as non-human agents
endowed with the ability to evaluate, rank, and reward or punish individuals’
actions and positions based on pre-programmed instructions <b>t</b>hat shape
social relationships </span><span
lang=EN-US>(Curchod, Patriotta, Cohen, &amp; Neysen, 2020</span><span
lang=EN-US>;<b> </b></span><span
lang=EN-US>Orlikowski &amp; Scott, 2008)</span><span lang=EN-US>. Algorithms,
however, are oftentimes compressed and hidden, and we do not encounter them in
the same way that we encounter traditional rules </span><span
lang=EN-US>(Lash, 2007</span><span lang=EN-US>; </span><span
lang=EN-US>Beer, 2017)</span><span lang=EN-US>. The increasing reliance on
algorithms as instruments for the regulation of social relationships, coupled
with the obscurity of algorithmic evaluation systems, is evidence of new yet
subtle ways of exercising power, which alters existing power-dependencies, e.g.,
through surveillance, online interaction, and so on </span><span
lang=EN-US>(Curchod et al., 2020</span><span lang=EN-US>;</span><span
lang=EN-US> Macnish, 2012)</span><span lang=EN-US>. Algorithms are therefore implicated
in the constitution and reproduction of power asymmetries that regulate
individuals’ behaviors and ensure their compliance with predefined standards,
which in turn can affect human agency </span><span
lang=EN-US>(Curchod et al., 2020)</span><span lang=EN-US>. </span><span
lang=EN-US>It is difficult, however, to identify ex-ante what the
socio-economic effects of scaling an AI-system will be </span><span
lang=EN-US>(Henfridsson et al., 2018</span><span lang=EN-US>; </span><span
lang=EN-US>Rai, Constantinides, &amp; Sarker, 2019)</span><span lang=EN-US>,
which warrants that extensive experimentation through application may be
necessary before AI-based technological diffusion and legitimization are likely
to take place. </span></p>

<p class=MsoNormal><span lang=EN-US>Institutional logics and institutional work
provide a foundation to understand the rationalities and practices of actors
that implement novel AI-agents, as well as the AI-agents’ systemic impact on
their surroundings through their socio-economic embeddedness. An analysis of
AI-agents predicated on institutional work and logics can be placed both at the
micro-level, seeking to understand the impact of individual AI-agents on specific
socio-economic practices, as well as at the meso-level, seeking to understand
how actors influence the legitimacy of AI applications in a field. That is, how
AI diffusion is adopted and accepted, or rejected, under varying socio-economic
and technological conditions. </span></p>

<p class=MsoListParagraph style='text-indent:-18.0pt'><b><span lang=EN-US>2.1<span
style='font:7.0pt "Times New Roman"'>&nbsp; </span></span></b><b><span
lang=EN-US>Digital Infrastructure</span></b></p>

<p class=MsoNormal><span lang=EN-US>Digital infrastructure is made from a
multitude of digital building blocks and is defined as the computing and
network resources that allow multiple stakeholders to orchestrate their service
and content needs (</span><span
lang=EN-US>Constantinides, Henfridsson, &amp; Parker 2018)</span><span
lang=EN-US>. Digital infrastructures are distinct from traditional infrastructures
because of their ability to collect, store, and make digital data available
across a large number of systems and devices simultaneously </span><span
lang=EN-US>(Constantinides et al., 2018)</span><span lang=EN-US>. </span><span
lang=EN-US>Examples of digital infrastructures include the Internet (Hanseth
and Lyytinen 2010, Monteiro 1998); data centers; open standards, e.g., IEEE
802.11 (Wi-Fi), as well as consumer devices such as smartphones.</span></p>

<p class=MsoNormal><span
lang=EN-US>Henfridsson et al. (2018: 90)</span><span lang=EN-US> refer to
“digital resources” as entities that serve as building blocks in the creation
and capture of value from information. While AI technologies are assembled as
digital building blocks, a distinction needs to be made between traditional software
systems (i.e., ERP, CRM, WordPress, etc.) and novel AI-systems (computer
vision, machine learning, etc.). This distinction is important as a new kind of
embedded agency is inherent in most AI systems, which render these as
“organizers,” “predictors,” or “controllers” of data flows that are captured by
digital infrastructures </span><span
lang=EN-US>(Russell &amp; Norvig, 2010)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>Most digital building blocks are made
accessible through online platforms or are proprietarily assembled through
open-source code. Digital building blocks are transformational due to the
innovative patterns that can be established through “use-recombination” </span><span
lang=EN-US>(Henfridsson et al., 2018)</span><span lang=EN-US>, while there
needs to be separate legitimacy for each building block, as well as collective
legitimacy for a new institutional arrangement to emerge </span><span
lang=EN-US>(Hinings et al., 2018)</span><span lang=EN-US>.<b> </b>It may, for
example, be that a platform-based building block holds legitimacy (e.g., a
cloud-based AI facial recognition-system) because it performs within a
predefined level of accuracy. However, for the organizational or wider
institutional arrangement to gain legitimacy, the embeddedness of the building
block into a socio-economic system needs to be accepted at a much broader level
of implementation. </span></p>

<p class=MsoNormal><span lang=EN-US>As digital building blocks are created by
engineers, and as humans are subject to bias </span><span
lang=EN-US>(Parasuraman &amp; Manzey, 2010)</span><span lang=EN-US>, this means
that the values of the designer can be “frozen into the code, effectively
institutionalizing those values’’ </span><span
lang=EN-US>(Macnish, 2012: 158)</span><span lang=EN-US>. Friedman and
Nissenbaum (1996) argue that bias in computer systems can arise in three
distinct ways, referring to (1) pre-existing social values found in the
‘‘social institutions, practices and attitudes’’ from which a technology
emerges, (2) technical constraints, and (3) emergent aspects that arise through
usage, which only can be known ex-post. The distinction between social and
technical bias has also been referred to as normative and epistemic concerns (</span><span
lang=EN-US>Mittelstadt et al., 2016)</span><span lang=EN-US> or structural and
functional risks (</span><span
lang=EN-US>Nuno et al., 2021)</span><span lang=EN-US>. Functional risks refer
to technical areas such as the design and operation of an AI system, including
datasets, bias, and performance issues, whereas structural risks refer to the
ethical implications of an AI system, including the societal effects of
automated decisions.</span></p>

<p class=MsoNormal><span lang=EN-US>Based on the distinction between functional
(i.e., technical) and structural (i.e., social) risks, three analytical
constructs, referring to technological maturity, data, and AI-autonomy, are
proposed below in order to signify the relative elaboration of AI-associated
digital infrastructure. </span></p>

<p class=MsoNormal><span lang=EN-US> <b>2.2 Technological Maturity, Data, &amp;
AI Autonomy</b></span></p>

<p class=MsoNormal><b><span lang=EN-US>Technological Maturity</span></b></p>

<p class=MsoNormal><span lang=EN-US>AI systems are subject to different degrees
of maturity, both in terms of the accuracy of the system </span><span
lang=EN-US>(Zhu, Yu, Halfaker, &amp; Terveen, 2018)</span><span lang=EN-US>, as
well as the elaboration of adjacent technological standards </span><span
lang=EN-US>(Garud, Jain, &amp; Kumaraswamy, 2002)</span><span lang=EN-US>. The
accuracy of an AI-model refers to whether it operates within a predefined
acceptable level of performance. In the case of autonomous vehicle safety, for instance,
an AI-controller is expected to hold the ability to locate persons and objects
from a distance of 100 meters with an accuracy of +/- 20 cm, within a false
negative rate of 1% and false-positive rate of 5% </span><span
lang=EN-US>(Grigorescu, Trasnea, Cocias, &amp; Macesanu, 2020)</span><span
lang=EN-US>. In some areas that involve high-stakes decisions (e.g., autonomous
driving, credit applications, judicial decisions, and medical recommendations),
high accuracy alone may not be sufficient, as these applications require
greater trust in AI services </span><span
lang=EN-US>(Arnold et al., 2019)</span><span lang=EN-US>. In high-risk areas,
it is important that the functional aspects of a model (i.e., accuracy, data,
etc.) are further elaborated through measures such as certification, testing,
auditing, as well as the elaboration of technological standards, which refers
to the institutional infrastructure of a field. </span></p>

<p class=MsoNormal><span lang=EN-US>Depending on the context and the area of
use, a range of quantitative measures can be used to evaluate the technological
maturity of an AI-induced field. Some suggestions include the measures of
scientific output, e.g., research papers, citations, and the intellectual
property rights that surround a given field. Important questions relate to
whether emerging algorithmic capabilities are under development and going
through stages of testing or already are being widely deployed by a small or a
large number of actors. For structural implications, it is important to ask
questions such as: how does the technological maturity and elaboration (of
immature/mature) AI-induced digital infrastructures affect a field? For
example, the implementation of chatbots, which may have performed with
sufficient accuracy under test environments, have proved to display racial
biases and prejudices, as the algorithm continues to learn during actual implementation,
which aggravates social harm for certain groups of the population </span><span
lang=EN-US>(Schlesinger, O’Hara, &amp; Taylor, 2018)</span><span lang=EN-US>.
The elements that are used to evaluate and decide whether an AI-system is
mature or immature are therefore dependent on its context of implementation,
which renders technical aspects alone insufficient when assessing the
technological maturity of AI-models and associated digital infrastructure. </span></p>

<p class=MsoNormal><span lang=EN-US>Several methods have been proposed to
evaluate predictive models, such as “model cards for model reporting” </span><span
lang=EN-US>(Mitchell et al., 2019)</span><span lang=EN-US>, “nutrition labels
for rankings” </span><span
lang=EN-US>(Yang et al., 2018)</span><span lang=EN-US>, “algorithmic impact
assessment” forms </span><span
lang=EN-US>(Reisman, Schultz, Crawford, &amp; Whittaker, 2018)</span><span
lang=EN-US>, as well as “fact sheets” </span><span
lang=EN-US>(Arnold et al., 2019)</span><span lang=EN-US>. These frameworks help
organizations establish new organizational practices that characterize
model-specifications in more coherent ways while paying special attention to
attributes such as accuracy, bias, consistency, transparency, interpretability,
and fairness, among others. </span></p>

<p class=MsoNormal><span lang=EN-US>At a general level, when dominant standards
are in place, and the accuracy of an AI-system is deemed safe, reliable, and
trustworthy, digital infrastructure is considered elaborate, and higher field
legitimacy is expected. If a technology is considered immature, inaccurate, or
insufficiently tested, the surrounding digital infrastructure would be
considered unelaborate. </span></p>

<p class=MsoNormal><b><span lang=EN-US>Data</span></b></p>

<p class=MsoNormal><span lang=EN-US>The nature of the data that feeds into any
AI-model or system is also of particular importance, and data can be classified
as being either sensitive (e.g., health-related) or non-sensitive (e.g.,
weather-related), and the nature of the data can be private (i.e., individual
data) or public (common/pooled data) </span><span
lang=EN-US>(Coyle, Diepeveen, Wdowin, Kay, &amp; Tennison, 2020)</span><span
lang=EN-US>. Data can also be biased, which makes AI systems prone to inherit
either individually coded forms of bias or biases that result from historical
or cultural practices, which are reflected in the training data, and could be
adopted by the algorithm </span><span
lang=EN-US>(Barocas &amp; Selbst, 2014</span><span lang=EN-US>; Diakopoulos,
2015). For an algorithm to be effective, its training data must be
representative of the communities that it impacts. The use of digital
infrastructures by individuals, machines, and communities, requires
institutions to negotiate how bits containing varying kinds of information
legitimately can be utilized and (re)arranged by organizations.</span></p>

<p class=MsoNormal><span lang=EN-US>Several methods have been proposed to
evaluate data as well as machine learning models under a variety of conditions.
For data, these include “data statements” </span><span
lang=EN-US>(Bender &amp; Friedman, 2018)</span><span lang=EN-US>, “datasheets
for data sets” </span><span
lang=EN-US>(Gebru et al., 2020)</span><span lang=EN-US>, and “nutrition labels
for data sets” </span><span
lang=EN-US>(Stoyanovich &amp; Howe, 2019)</span><span lang=EN-US>, which seek
to evaluate the data that goes into a model across training, testing, and
post-implementation scenarios. </span></p>

<p class=MsoNormal><span lang=EN-US>Sound data practices that are transparent,
well-documented, and privacy-preserving, are generally associated with a more
elaborate digital infrastructure. Data practices that are biased, undocumented,
or otherwise disputed could be considered a sign of unelaborate digital
infrastructure.</span></p>

<p class=MsoNormal><b><span lang=EN-US>Autonomy</span></b></p>

<p class=MsoNormal><span lang=EN-US>AI-agents hold varying degrees of autonomy
to act, while the (explorative) actions of an autonomous learning agent may not
always be known and can be subject to change depending on the data that is fed
into the model </span><span
lang=EN-US>(Amodei et al., 2016)</span><span lang=EN-US>. An AI-agent can have
limited or extensive autonomy to make decisions, while the decisions of an AI
agent can have a lenient (e.g., recommender engine, smart speaker) or a severe
(e.g., autonomous vehicle, incarceration system, facial recognition) impact on
individuals as well as its surroundings, if the algorithm is inaccurate, fails,
or is otherwise at fault. This could include aspects such as excessive
collection of data or unwilling intrusion of privacy in the case of facial
recognition systems, for example. The categorization of an agent’s autonomy,
therefore, includes its ability to act, as well as the possible ramifications
of its actions. The perceived risk of an AI agent can be understood as the
probability that a disruptive event occurs, multiplied by the severity of
potential harm to an individual or form of organization </span><span
lang=EN-US>(Nuno et al., 2021)</span><span lang=EN-US>. The definition of
“harm” and the computation of probability and severity is context-dependent and
varies across sectors. For instance, the impact of an autonomous decision in
medical diagnosis or in autonomous vehicles would, arguably, be greater than
that of a product recommendation system </span><span
lang=EN-US>((PDPC) &amp; (IMDA), 2020)</span><span lang=EN-US>. Relevant questions
include: what risks may be present in model usage (?), as well identification
of the potential recipients, likelihood, and magnitude of harms </span><span
lang=EN-US>(Amodei et al., 2016)</span><span lang=EN-US>. Where risks are taken
into consideration and are sufficiently mitigated in relation to avoiding any
potential harms, the digital infrastructure could be considered elaborate.</span></p>

<p class=MsoNormal><span lang=EN-US>The elaboration of AI-associated digital infrastructure
across the constructs of technological maturity, data, and AI-autonomy, remain
subject to both qualitative and quantitative judgments and measures, which are
field-dependent and linked to idiosyncrasies across functional (technical) as
well as structural (ethical) risks and considerations.</span></p>

<p class=MsoNormal><b><span lang=EN-US> 2.3 Governance</span></b></p>

<p class=MsoNormal><span lang=EN-US>Since field-level advancements in AI are
context-dependent, this means that the existing institutional infrastructure
and logics negotiates the actual impact that a technology is allowed to have
within a given social context, which differs across geographies. In other
words, t</span><span lang=EN-US>he flexibility of a digital infrastructure is
often restricted by socio-technical and regulatory arrangements (e.g.,
restrictions on autonomous vehicles, regulations on the use of patient’s
medical data, etc.). Oftentimes, layered and interoperable standards and common
definitions of application and service interfaces guide the use and growth of
digital infrastructures </span><span
lang=EN-US>(Tilson, Lyytinen, &amp; Sørensen, 2010)</span><span lang=EN-US> and
are necessary for digital infrastructures wider process of
institutionalization. As large technology companies usually are the leading
innovators of a field, these also carry a crucial weight in the direction of
new technology standards </span><span
lang=EN-US>(Pisano &amp; Teece, 2007)</span><span lang=EN-US>, which generally
affects how an industry or a field continues to evolve. </span><span
lang=EN-US>Typically, private actors orchestrate digital infrastructures, which
brings issues to the forefront, such as the challenge of establishing a governance
system, reproducing social order, and incorporating aspects of value
appropriation and control </span><span
lang=EN-US>(Botzem &amp; Dobusch, 2012</span><span lang=EN-US>; </span><span
lang=EN-US>Djelic &amp; Sahlin-Andersson, 2006</span><span lang=EN-US>; </span><span
lang=EN-US>Garud et al., 2002</span><span lang=EN-US>; </span><span
lang=EN-US>Garud &amp; Karnøe, 2003</span><span lang=EN-US>; </span><span
lang=EN-US>Raynard, 2016)</span><span lang=EN-US> </span></p>

<p class=MsoNormal><span lang=EN-US>The process that renders digital
infrastructures institutional occurs when innovators infuse specific norms,
values, logics, as well as forms of governance and technological control into
the infrastructure, and as the infrastructure becomes more widely adopted and
legitimized over time </span><span
lang=EN-US>(Gawer &amp; Phillips, 2013</span><span lang=EN-US>; </span><span
lang=EN-US>Orlikowski, 2007</span><span lang=EN-US>; </span><span
lang=EN-US>Orlikowski &amp; Scott, 2008)</span><span lang=EN-US>. </span><span
lang=EN-US>Digital institutional infrastructure can thus be viewed as the integration
of digital infrastructure and institutional infrastructure, which is defined as
standard-setting digital (AI) technologies that enable, constrain and
coordinate numerous actors’ actions and interactions in ecosystems, fields, or
industries (</span><span
lang=EN-US>Hinings et al., 2018</span><span lang=EN-US>; </span><span
lang=EN-US>Nambisan, 2016</span><span lang=EN-US>; </span><span
lang=EN-US>Star, 2002</span><span lang=EN-US>; </span><span
lang=EN-US>Tilson et al., 2010</span><span lang=EN-US>; </span><span
lang=EN-US>Timmermans &amp; Epstein, 2010</span><span lang=EN-US>; </span><span
lang=EN-US>Yoo et al., 2010)</span><span lang=EN-US>. Digital infrastructures,
however, tend to emerge more rapidly than institutional infrastructures (e.g.,
laws and regulations), which is commonly referred to as the pacing problem </span><span
lang=EN-US>(Hagemann, Huddleston, &amp; Thierer, 2018)</span><span lang=EN-US>.
This may create extensive issues if negative externalities are associated with
fast-moving technological implementation that is at odds with existing structures
or norms for certain actors or groups of a population. </span></p>

<p class=MsoListParagraph style='text-indent:-18.0pt'><b><span lang=EN-US>3.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp; </span></span></b><b><span
lang=EN-US>A Conceptual Framework for Understanding AI-Induced Field Change </span></b></p>

<p class=MsoNormal><span lang=EN-US>By integrating the insights from
institutional theory (work, logics) with information systems theory (digital
infrastructure), a novel framework for analyzing AI-induced field change is
proposed (Table 1). The framework builds on </span><span
lang=EN-US>Zietsma et al.’s (2017)</span><span lang=EN-US> conceptualization of
pathways of change, which hypothesizes how actors drive change across different
sets of field circumstances. The proposed framework extends </span><span
lang=EN-US>Zietsma et al.’s (2017)</span><span lang=EN-US> work through
incorporating the notion of AI-associated digital infrastructures, which has
implications for the structure and organization of (digital) institutions going
forward.</span></p>

<p class=MsoNormal><span lang=EN-US>The proposed framework first considers
varying actors and their position in a field before elaborating on these
ability to affect the direction of a field, either through the introduction of
a new technology, regulation, or a social movement, for example. Next, the
relationship among actors as well as their coherency in terms of logics is
considered. When logics are unitary, greater field alignment is expected,
whereas competing logics means that a field is unsettled. The elaboration of
institutional infrastructure is considered by looking at the practices and
actions of individual actors as well as organizations in terms of </span><span
lang=EN-US style='color:black'>creating, maintaining, and disrupting
institutions over time. The notion of field structuring events is particularly
important, both in terms of logic formation or disruption, as well as for the elaboration
of the institutional infrastructure of a field. </span></p>

<p class=MsoNormal><span lang=EN-US>The AI-associated digital infrastructure of
a field is signified by the proposed constructs of technological maturity,
data-specification, and the relative autonomy of an AI-system. Technological
maturity refers to the perceived accuracy of an AI agent, as well as the
elaboration of areas pertaining to standards, research, IP, etc. The data linked
to a model is another important source of institutional legitimacy, both
functionally (e.g., non-biased data) as well as structurally (e.g., how an
organization is engaged in practices of data collection and usage). Autonomy
refers to the relative impact of an AI agent on its general environment, as
well as its potentials for exacerbating structural risks and create harm. At
last, the governance of a field, as well as the mechanisms that guide
algorithmic implementation, are considered. </span></p>

<p class=MsoNormal><b><span lang=EN-US style='font-size:10.0pt;line-height:
107%'>Table 1:</span></b><span lang=EN-US style='font-size:10.0pt;line-height:
107%'> <b>Framework for Analyzing AI-Induced Field Change and Legitimization:</b>
</span></p>

<div align=center>

<table class=MsoNormalTable border=0 cellspacing=0 cellpadding=0 width=607
 style='width:455.0pt;border-collapse:collapse'>
 <tr style='height:15.75pt'>
  <td width=607 colspan=2 valign=bottom style='width:455.0pt;border:solid windowtext 1.0pt;
  border-right:solid black 1.0pt;background:#BFBFBF;padding:0cm 5.4pt 0cm 5.4pt;
  height:15.75pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><b><span lang=EN-US style='font-size:10.0pt;color:black'>ACTORS</span></b></p>
  </td>
 </tr>
 <tr style='height:36.2pt'>
  <td width=607 colspan=2 valign=top style='width:455.0pt;border-top:none;
  border-left:solid windowtext 1.0pt;border-bottom:none;border-right:solid black 1.0pt;
  background:white;padding:0cm 5.4pt 0cm 5.4pt;height:36.2pt'>
  <p class=MsoListParagraphCxSpFirst style='margin-bottom:0cm;text-indent:-18.0pt;
  line-height:normal'><span lang=EN-US style='font-size:10.0pt;font-family:
  Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><span lang=EN-US style='font-size:10.0pt;color:black'>Subject
  position: central, middle status, and peripheral actors</span></p>
  <p class=MsoListParagraphCxSpLast style='margin-bottom:0cm;text-indent:-18.0pt;
  line-height:normal'><span lang=EN-US style='font-size:10.0pt;font-family:
  Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><span lang=EN-US style='font-size:10.0pt;color:black'>Characterized
  by roles or functions, i.e., field-structuring or governing organizations,
  formal governance units, field coordinators, etc.</span></p>
  </td>
 </tr>
 <tr style='height:15.75pt'>
  <td width=607 colspan=2 valign=bottom style='width:455.0pt;border:solid windowtext 1.0pt;
  border-right:solid black 1.0pt;background:#BFBFBF;padding:0cm 5.4pt 0cm 5.4pt;
  height:15.75pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><b><span lang=EN-US style='font-size:10.0pt;color:black'>DIGITAL
  INSTITUTIONAL INFRASTRUCTURE</span></b></p>
  </td>
 </tr>
 <tr style='height:15.75pt'>
  <td width=607 colspan=2 valign=bottom style='width:455.0pt;border-top:none;
  border-left:solid windowtext 1.0pt;border-bottom:solid windowtext 1.0pt;
  border-right:solid black 1.0pt;padding:0cm 5.4pt 0cm 5.4pt;height:15.75pt'>
  <p class=MsoNormal style='margin-bottom:0cm;line-height:normal'><span
  lang=EN-US style='font-size:10.0pt'>S</span><span lang=EN-US
  style='font-size:10.0pt;color:black'>tandard-setting digital technologies
  that enable, constrain, and coordinate numerous actors’ actions and
  interactions in ecosystems, fields, or industries </span><span lang=EN-US
  style='font-size:10.0pt'>(</span><span
  lang=EN-US style='font-size:10.0pt'>Hinings et al., 2018</span><span
  lang=EN-US style='font-size:10.0pt'>).</span><span lang=EN-US
  style='font-size:10.0pt;color:black'> <br clear=all style='page-break-before:
  always'>
  <br clear=all style='page-break-before:always'>
  <br clear=all style='page-break-before:always'>
  <br clear=all style='page-break-before:always'>
  </span></p>
  </td>
 </tr>
 <tr style='height:15.75pt'>
  <td width=287 valign=bottom style='width:215.0pt;border:solid windowtext 1.0pt;
  border-top:none;background:#D9D9D9;padding:0cm 5.4pt 0cm 5.4pt;height:15.75pt'>
  <p class=MsoNormal style='margin-bottom:0cm;line-height:normal'><b><span
  lang=EN-US style='font-size:10.0pt;color:black'>INSTITUTIONAL INFRASTRUCTURE</span></b></p>
  </td>
  <td width=320 valign=bottom style='width:240.0pt;border-top:none;border-left:
  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  background:#D9D9D9;padding:0cm 5.4pt 0cm 5.4pt;height:15.75pt'>
  <p class=MsoNormal style='margin-bottom:0cm;line-height:normal'><b><span
  lang=EN-US style='font-size:10.0pt;color:black'>DIGITAL INFRASTRUCTURE</span></b></p>
  </td>
 </tr>
 <tr style='height:84.05pt'>
  <td width=287 valign=top style='width:215.0pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt;height:84.05pt'>
  <p class=MsoNormal style='margin-bottom:0cm;line-height:normal'><span
  lang=EN-US style='font-size:10.0pt'>Established through activities such as:
  certifying, assuring, and reporting against principles, codes, rules, and
  standards, as well as through the formation of new associations and networks
  among organizations, including official rules and regulations </span><span
  lang=EN-US style='font-size:10.0pt'>(Waddock, 2008)</span><span lang=EN-US
  style='font-size:10.0pt'>.</span><span lang=EN-US> </span><span lang=EN-US
  style='font-size:10.0pt'> </span></p>
  <p class=MsoListParagraphCxSpFirst style='margin-bottom:0cm;line-height:normal'><span
  lang=EN-US style='font-size:10.0pt;color:black'>&nbsp;</span></p>
  <p class=MsoListParagraphCxSpMiddle style='margin-bottom:0cm;text-indent:
  -18.0pt;line-height:normal'><span lang=EN-US style='font-size:10.0pt;
  font-family:Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><b><span lang=EN-US style='font-size:10.0pt;color:black'>Logics</span></b><span
  lang=EN-US style='font-size:10.0pt;color:black'>: what are the relationships
  among individuals and organizations in the field? Are logics competing or
  unitary? Are they based on market, social, or other rationalities?<br
  clear=all style='page-break-before:always'>
  <br clear=all style='page-break-before:always'>
  </span></p>
  <p class=MsoListParagraphCxSpLast style='margin-bottom:0cm;text-indent:-18.0pt;
  line-height:normal'><span lang=EN-US style='font-size:10.0pt;font-family:
  Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><b><span lang=EN-US style='font-size:10.0pt;color:black'>Work</span></b><span
  lang=EN-US style='font-size:10.0pt;color:black'>: what are some of the practices
  and actions of individuals and organizations that have implications for
  creating, maintaining, and disrupting institutions over time? <br clear=all
  style='page-break-before:always'>
  <br clear=all style='page-break-before:always'>
  </span><span lang=EN-US style='font-size:10.0pt'>What effect does
  institutional change have on hierarchies of status and influence and
  subsequent power relations? </span><span lang=EN-US style='font-size:10.0pt;
  color:black'>What are some of the field structuring events? </span></p>
  </td>
  <td width=320 valign=top style='width:240.0pt;border-top:none;border-left:
  none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt;height:84.05pt'>
  <p class=MsoNormal style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;
  margin-left:18.0pt;line-height:normal'><span lang=EN-US style='font-size:
  10.0pt'>Established from a multitude of digital building blocks, defined as
  the computing and network resources that allow multiple stakeholders to
  orchestrate their service and content needs (</span><span
  lang=EN-US style='font-size:10.0pt'>Constantinides et al., 2018)</span><span
  lang=EN-US style='font-size:10.0pt'>. </span></p>
  <p class=MsoListParagraphCxSpFirst style='margin-bottom:0cm;line-height:normal'><span
  lang=EN-US style='font-size:10.0pt;color:black'>&nbsp;</span></p>
  <p class=MsoListParagraphCxSpMiddle style='margin-bottom:0cm;text-indent:
  -18.0pt;line-height:normal'><span lang=EN-US style='font-size:10.0pt;
  font-family:Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><b><span lang=EN-US style='font-size:10.0pt;color:black'>Technological
  Maturity</span></b><span lang=EN-US style='font-size:10.0pt;color:black'>:
  refers to the elaboration of hardware and software-based infrastructures and
  associated technological standards. Includes the perceived accuracy, safety,
  and reliability of an AI system/agent.</span></p>
  <p class=MsoListParagraphCxSpMiddle style='margin-bottom:0cm;text-indent:
  -18.0pt;line-height:normal'><span lang=EN-US style='font-size:10.0pt;
  font-family:Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><b><span lang=EN-US style='font-size:10.0pt;color:black'>Data</span></b><span
  lang=EN-US style='font-size:10.0pt;color:black'>: is the data utilized
  sensitive or non-sensitive? Is it private or publicly available? Is it
  centralized, and who has ownership? </span></p>
  <p class=MsoListParagraphCxSpLast style='margin-bottom:0cm;text-indent:-18.0pt;
  line-height:normal'><span lang=EN-US style='font-size:10.0pt;font-family:
  Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><b><span lang=EN-US style='font-size:10.0pt;color:black'>Autonomy</span></b><span
  lang=EN-US style='font-size:10.0pt;color:black'>: refers to whether the
  AI-agent holds limited or extensive autonomy to act and whether the agent’s
  actions have a negligible or a considerable impact on its environment and
  surroundings. <br clear=all style='page-break-before:always'>
  <br clear=all style='page-break-before:always'>
  <br clear=all style='page-break-before:always'>
  <br clear=all style='page-break-before:always'>
  </span></p>
  </td>
 </tr>
 <tr style='height:15.75pt'>
  <td width=607 colspan=2 valign=bottom style='width:455.0pt;border-top:none;
  border-left:solid windowtext 1.0pt;border-bottom:solid windowtext 1.0pt;
  border-right:solid black 1.0pt;background:#BFBFBF;padding:0cm 5.4pt 0cm 5.4pt;
  height:15.75pt'>
  <p class=MsoNormal align=center style='margin-bottom:0cm;text-align:center;
  line-height:normal'><b><span lang=EN-US style='font-size:10.0pt;color:black'>GOVERNANCE</span></b></p>
  </td>
 </tr>
 <tr style='height:37.5pt'>
  <td width=607 colspan=2 valign=top style='width:455.0pt;border-top:none;
  border-left:solid windowtext 1.0pt;border-bottom:solid windowtext 1.0pt;
  border-right:solid black 1.0pt;background:white;padding:0cm 5.4pt 0cm 5.4pt;
  height:37.5pt'>
  <p class=MsoListParagraphCxSpFirst style='margin-bottom:0cm;text-indent:-18.0pt;
  line-height:normal'><span lang=EN-US style='font-size:10.0pt;font-family:
  Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><span lang=EN-US style='font-size:10.0pt;color:black'>Combinations
  of public and private, formal and informal systems that exercise control
  within a field. </span></p>
  <p class=MsoListParagraphCxSpMiddle style='margin-bottom:0cm;text-indent:
  -18.0pt;line-height:normal'><span lang=EN-US style='font-size:10.0pt;
  font-family:Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><span lang=EN-US style='font-size:10.0pt;color:black'>Units and
  processes that ensure compliance with rules and facilitate ‘the overall
  smooth functioning and reproduction of the system (e.g., standards,
  regulations, reward systems, and social control agents that monitor and
  enforce these).</span></p>
  <p class=MsoListParagraphCxSpLast style='margin-bottom:0cm;text-indent:-18.0pt;
  line-height:normal'><span lang=EN-US style='font-size:10.0pt;font-family:
  Symbol;color:black'>·<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><span lang=EN-US style='font-size:10.0pt;color:black'>Governance
  can differ within and between fields, as well as across geographies, e.g.,
  countries.</span></p>
  </td>
 </tr>
</table>

</div>

<p class=MsoNormal><span lang=EN-US>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN-US>Based on coherency in logics (unitary,
competing)(</span><span
lang=EN-US>Hinings et al., 2017)</span><span lang=EN-US>, and the elaboration
of institutional infrastructure (high, low)</span><span
lang=EN-US>(Greenwood et al., 2011)</span><span lang=EN-US>, a four-fold
classification of field conditions is produced around whether there are settled
or unsettled logic prioritizations and limited or elaborated digital and
institutional infrastructure (Figure 1)</span><span
lang=EN-US>(Zietsma et al., 2017)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>Where digital and institutional
infrastructure is highly elaborate, and there is a unitary dominant logic
within the field, the field can be described as established and relatively
stable, i.e., the institutional infrastructure is coherent </span><span
lang=EN-US>(Zietsma et al., 2017)</span><span lang=EN-US>. Formal governance
and informal infrastructure elements are elaborate and likely to reinforce each
other, leading to a coherent sense of what is legitimate or not within the
organizational field </span><span
lang=EN-US>(Greenwood, Suddaby, &amp; Hinings, 2002</span><span lang=EN-US>; </span><span
lang=EN-US>Zietsma &amp; Lawrence, 2010)</span><span lang=EN-US>.  </span></p>

<p class=MsoNormal><span lang=EN-US>In fields where there is highly elaborate
institutional infrastructure but competing logics (low coherency), there could
be multiple formal governance and digital and institutional infrastructure
arrangements </span><span
lang=EN-US>(Zietsma et al., 2017)</span><span lang=EN-US>. These arrangements
may be in conflict with one another or compete for dominance, which makes the
field contested </span><span
lang=EN-US>(Reay &amp; Hinings, 2005</span><span lang=EN-US>; </span><span
lang=EN-US>Rao, Morrill, &amp; Zald, 2000)</span><span lang=EN-US>. Contested
refers both to competing digital infrastructures (e.g., technological standards,
varying models, and levels of algorithmic accuracy), as well as to stakeholders
opposing views. </span></p>

<p class=MsoNormal><span lang=EN-US>&nbsp;</span></p>

<p class=MsoNormal><b><span lang=EN-US style='font-size:10.0pt;line-height:
107%'>Figure 1: Digital / Institutional Infrastructure and Logics: Framework
for Field-Level Change (modified from </span></b><b><span
lang=EN-US style='font-size:10.0pt;line-height:107%'>Zietsma et al. (2017)</span></b><b><span
lang=EN-US style='font-size:10.0pt;line-height:107%'>.</span></b></p>

<p class=MsoNormal><span lang=EN-US><img width=451 height=332 id="Picture 1"
src="wp.fld/image001.png"></span></p>

<p class=MsoNormal><span lang=EN-US>Fields with low coherency and limited
elaboration of digital and institutional infrastructure are described as
fragmented, with competing conceptions of what is legitimate. Fields may be
fragmented if they emerge in intermediate positions (e.g., biotechnology), which
draws on logics and practices from diverse but neighboring fields </span><span
lang=EN-US>(Powell &amp; Sandholtz, 2012)</span><span lang=EN-US>. A field may
also be fragmented as new actors enter an existing field with innovative ideas
and designs about products, courses of action, behaviors, as well as new structures
and ways of organization </span><span
lang=EN-US>(Patvardhan, Gioia, &amp; Hamilton, 2015)</span><span lang=EN-US>.
In the field of facial recognition technology, for instance, there are multiple
competing logics that move across varying stakeholders and demonstrate incoherent
views over technological accuracy, as well as the technology’s inherent ability
to enhance public safety. Many differing views paired with a limited (but
expanding) digital infrastructure situates the field in the fragmented quadrant.</span></p>

<p class=MsoNormal><span lang=EN-US>When infrastructure has a low degree of
elaboration but a high degree of coherency in terms of unitary logics, the
field is described as emerging or aligning </span><span
lang=EN-US>(Hinings et al., 2017)</span><span lang=EN-US>. While the lack of
digital and institutional infrastructure in an emerging field may create
considerable room for experimentation and change, it may also limit field
members’ ability to define and acquire legitimacy and thus contributes to
ambiguity, and potentially, the need to draw on ill-suited infrastructure from
adjacent fields. One example could be the emergence of autonomous vehicles,
drawing on existing legal frameworks in terms of liability, which, however, are
ill-suited in terms of covering the accompanying change in agency and
responsibility.  </span></p>

<p class=MsoNormal><span lang=EN-US>Categorizing a field’s present condition as
well as its potential trajectories enables us to get a deeper understanding of
possible areas of contestation, fragmentation, or alignment, as well as what it
takes for an AI-induced field to grow established over time. Before these
conditions are further discussed in section 5, the following section </span><span
lang=EN-US style='color:black'>applies </span><span lang=EN-US>the developed
framework (Table 1) to</span><span lang=EN-US style='color:black'> the field of
facial recognition technologies in the United States. The application briefly
illustrates the utility of the framework in terms of assessing
field-elaboration, while future studies may apply the framework to analyze
case-studies at greater length. </span></p>

<p class=MsoNormal style='margin-bottom:13.5pt;line-height:normal'><b><span
lang=EN-US>4. Analyzing AI-Induced Field Change and Legitimization: Facial
Recognition Technology</span></b></p>

<p class=MsoNormal style='line-height:normal'><b><span lang=EN-US>4.1 Actors</span></b></p>

<p class=MsoNormal style='line-height:normal'><span lang=EN-US>The
proliferation of facial recognition technologies in the United States has been supported
by large technology companies, which are the central actors of the field (e.g.,
Apple, Amazon, Google, Microsoft, IBM). While these companies provide their own
applications directly to the market, they also modularize facial-recognition
technologies and make them accessible for complementors on their platforms.
This makes them field structuring organizations since the modularization of
FRT-systems embodies best-practices and de-facto industry standards, which
other companies align with. Central actors include adopters of FRT-systems,
while many of these are U.S public sector agencies. </span><span lang=EN-US>Contractors
that specialize in delivering FRT-technology to law-enforcement agencies, as
well as the National Institute of Standards and Technology (NIST), hold
intermediate positions. </span><span lang=EN-US>Peripheral actors include
multistakeholder organizations such as the Partnership on AI, non-profit
research organizations such as the Center for Data and Society, as well as
research institutes such as The AI Now Institute (NYU). These actors affect the
field through public reports and commentaries, paying special attention to
issues of technological implementation and social ramifications. Peripheral
actors also include opponents of FRT-systems, both in the form of activists, as
well as civil society organizations such as </span><span lang=EN-US>The
American Civil Liberties Union (ACLU)</span><span lang=EN-US>. </span></p>

<p class=MsoNormal style='line-height:normal'><b><span lang=EN-US>4.2 Logics</span></b></p>

<p class=MsoNormal style='line-height:normal'><span lang=EN-US>The dominant
logics behind FRT’s has been driven by private sector companies focused on
gaining market share. </span><span lang=EN-US>The logics behind adoption is motivated
by enhancing measures of public safety, e.g., in terms of identifying
criminals, </span><span lang=EN-US>screening travelers, and processing border
immigration. </span><span lang=EN-US>Both logics are highly contested by
peripheral actors </span><span lang=EN-US>(e.g., company activists and civil
rights organizations)</span><span
lang=EN-US>(Hao, 2020b)</span><span lang=EN-US>, citing that inaccurate
technologies hold the potential of exacerbating racial and social biases and
inequities. This signifies that emergent dominant logics are </span><span
lang=EN-US>at odds with existing social arrangements, including structures of power
and governance, which makes the technology heavily resisted</span><span
lang=EN-US> </span><span lang=EN-US>(Furnari, 2016)</span><span lang=EN-US>. </span></p>

<p class=MsoNormal style='line-height:normal'><b><span lang=EN-US>4.3 Work: Field
Structuring Events</span></b><span lang=EN-US> </span></p>

<p class=MsoNormal style='line-height:normal'><span lang=EN-US>In 2019, the
local government of San Francisco became the first city in the United States to
ban the use of FRT’s by local agencies. In the spring of 2020, nationwide protests
against police brutality and racial profiling caused several central actors
(IBM, Amazon, Microsoft) to stop providing FRT-technologies to law enforcement
agencies altogether. IBM </span><span lang=EN-US>called for “a national
dialogue on whether and how facial recognition technology should be deployed by
domestic law enforcement agencies” </span><span
lang=EN-US>(Krishna, 2020, p.1)</span><span lang=EN-US>, and Amazon announced a
one-year moratorium on police use of its facial recognition technology, giving
policymakers time to set appropriate rules around the use of the technology.
Microsoft declared that it would not sell FRT-technology to police departments
in the United States until a federal law that regulates the technology is
formulated.&nbsp;These actions by some of the central actors in the field signal
that the existing institutional infrastructure remains inadequate in terms of
governing and addressing the current expansion of FRT-related digital
infrastructure. This indicates that even as central actors on the procurement
side include many public sector agencies, the necessary institutional
infrastructure to guide potential ramifications of immature technological adoption
has not yet been formulated. Greater alignment between stakeholders across
industry, government, and civil society, is currently needed in order to secure
ongoing legitimacy as well as greater field-level elaboration and use of facial
recognition technologies.  </span></p>

<p class=MsoNormal style='line-height:normal'><b><span lang=EN-US>4.4 Technological
Maturity</span></b><span lang=EN-US> </span></p>

<p class=MsoNormal style='line-height:normal'><span lang=EN-US>In terms of
technological maturity, v</span><span lang=EN-US>erification algorithms have
achieved accuracy scores of up to&nbsp;<a
href="https://pages.nist.gov/frvt/html/frvt11.html"><span style='color:windowtext;
text-decoration:none'>99.97%</span></a>&nbsp;on standard assessments like the
National Institute for Standards and Technology (NIST) Facial Recognition
Vendor Test </span><span
lang=EN-US>(NIST, 2020)</span><span lang=EN-US>. For identification-systems,
error rates tend to climb when high-quality images are replaced with the feed from
live cameras that normally are utilized in public spaces. Aging is another
factor that affects error rates, while accuracies of FRT-systems differ
considerably across </span><span lang=EN-US>gender and race. The context, i.e.,
the specific area of implementation and use, can therefore be said to have
wide-reaching consequences for the accuracy-rates of individual FRT-systems.</span></p>

<p class=MsoNormal style='line-height:normal'><b><span lang=EN-US>4.5 Data</span></b><span
lang=EN-US> </span></p>

<p class=MsoNormal style='line-height:normal'><span lang=EN-US>Issues of
legitimacy are also inherent in relation to the kinds of data that are being
used for training FRT-algorithms. Many databases rely on publicly available
face-annotated data, which in some cases are scraped directly from social media
platforms and have raised issues over privacy and consent </span><span
lang=EN-US>(Hao, 2020a)</span><span lang=EN-US>. The company Clearview has, for
example, assembled a database containing some 3 billion images, where many have
been scraped from public-facing social media platforms </span><span
lang=EN-US>(Hill, 2020a)</span><span lang=EN-US>. This raises concerns about the
legitimacy of data rights and usage, as well as the ability of existing
institutional infrastructure to provide, and safeguard, associated rights. The quantity
of data is in many cases important for algorithmic training, as well as for retaining
levels of accuracy post-deployment, which means that there is an inherent
incentive for private, as well as for public-sector adopters, to amass rich
databases (e.g., new biometric data), in order to increase and continue to ensure
the accuracy of a given system. </span><span lang=EN-US>In several states (e.g.,
Texas, Florida, Illinois), the FBI is allowed to use facial recognition
technology to scan through&nbsp;the Department of Motor Vehicles (DMV)
database&nbsp;of drivers’ license photos </span><span
lang=EN-US>(Ghaffary &amp; Molla, 2019)</span><span lang=EN-US> in order to
generate a more coherent centralized biometric database. As these kinds of data
contain personal information, they are classified as being sensitive and
vulnerable, both in terms of misuse as well as in relation to cybersecurity
breaches and possible identity theft </span><span
lang=EN-US>(Coyle et al., 2020)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal style='line-height:normal'><b><span lang=EN-US>4.6 Autonomy</span></b><span
lang=EN-US> </span></p>

<p class=MsoNormal style='line-height:normal'><span lang=EN-US>AI in facial
recognition-systems is perceived as a new kind of social control agent, which
may exert autonomy over law-enforcement officers in relation to issuing arrest
orders. If the accuracy of a system is flawed, an officers’ actions are likely
to cause social harm whenever an innocent citizen is arrested (</span><span
lang=EN-US>Hill, 2020b)</span><span lang=EN-US>. T</span><span lang=EN-US>he
adoption of facial recognition systems for use in law enforcement alters
existing power dependencies, as officers have to trust in, and act on, the
information that is rendered to them by the system. </span><span lang=EN-US>Facial
recognition systems are thus shaping entirely new practices and forms of
organization in which the autonomy of the AI-agent is dependent on the delivery
of accurate information, which could reinforce a drive towards
data-centralization. </span></p>

<p class=MsoNormal style='line-height:normal'><b><span lang=EN-US>4.7 Governance</span></b></p>

<p class=MsoNormal style='line-height:normal'><span lang=EN-US>The field of
facial recognition technology is fragmented and exhibits low coherency and
limited elaboration in terms of institutional infrastructure. A lack of
governance is most readily seen in the absence of coherent rules and
regulations, while the field is currently going through a shift from self-regulation
towards more formalized governance arrangements. This shift has been called for
by peripheral actors, and more recently also by central actors from the private
sector, which demands new rules to guide legitimate implementation going
forward. The case of facial recognition technologies used by law-enforcement
highlights the critical role of culture and politics involved in the
organization of markets and in creating the governing ‘rules of the game’ </span><span
lang=EN-US>(North, 1990</span><span lang=EN-US>; </span><span
lang=EN-US>Fligstein, 2001</span><span lang=EN-US>; </span><span
lang=EN-US>Fligstein &amp; McAdam, 2013)</span><span lang=EN-US> </span></p>

<p class=MsoListParagraph style='text-indent:-18.0pt'><b><span lang=EN-US>5.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp; </span></span></b><b><span
lang=EN-US>Pathways of Change: How AI-Fields Move and Gain Legitimacy </span></b></p>

<p class=MsoNormal><span lang=EN-US>Field-level change is brought about by a
number of distinct triggers such as competition, and as new technologies,
business models, or organizational structures are introduced, which change
existing relations as well as practices (mimetic isomorphism) </span><span
lang=EN-US>(Furnari, 2014</span><span lang=EN-US>; </span><span
lang=EN-US>Garud, 2008</span><span lang=EN-US>; Munir &amp; Phillips, 2005). Pathways
of change suggest that there are some commonalities to how fields are likely to
evolve and where obstacles to legitimization and institutionalization may be
found. In order to understand how fields move between states, we need to pay
special attention to the scope of change (i.e., which elements change and how
much changes) </span><span
lang=EN-US>(Maguire &amp; Hardy, 2009)</span><span lang=EN-US>, as well as the
pace of change (i.e., the speed at which a field moves from one condition to
another)</span><span
lang=EN-US>(Amis, Slack, &amp; Hinings, 2004)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>In the case of facial recognition
technologies, the field is currently moving from the fragmented towards the
contested quadrant, as the number of use-cases (e.g., public surveillance,
airport check-ins, smartphones, doorbells, etc.) continues to expand, based on
rising technological maturity (e.g., accuracy, standards). While digital infrastructures
are expanding, the field continues to be represented by incoherent logics and sparse
institutional infrastructure, however. For example, verification-based FRT’s (e.g.,
unlocking a smartphone) is already a well-established practice and exhibits
legitimate institutionalized functions. Identification-based FRT’s (e.g., public
surveillance), on the other hand, are more likely to stay contested due to
having a lower degree of algorithmic accuracy, which is paired with more severe
social impacts linked to the autonomy of AI-agents, and how these alter
existing power structures. In order for the field, as a whole, to grow more
established, a shift from self-regulation towards formalized governance
arrangements and greater alignment and coherency in terms of logics is needed
(lack of normative isomorphism). In more authoritarian settings, such as in China,
the field of facial recognition is already on its way to becoming established.
This signifies that a country’s socio-political setting informs its institutional
infrastructure, which has important implications for a technology’s path towards
legitimization as well as processes of institutionalization. </span></p>

<p class=MsoNormal><span lang=EN-US>A pathway that moves from an aligning or
emerging field condition to an established condition usually involves a process
of convergence, which is commonly observed in the institutionalization of most
fields </span><span
lang=EN-US>(see, e.g., Munir &amp; Phillips, 2005)</span><span lang=EN-US>. The
field of autonomous vehicles (AV) is characterized by its emerging digital and
institutional infrastructure, which has a low degree of elaboration but some
coherency in terms of logics. While the field is currently aligning at a
relatively slow pace, it develops in extension of an existing field, which has
been elaborated over decades. Large parts of the existing infrastructure are
challenged, however, through the introduction of novel AI-agents and a transfer
in autonomy from humans to machines. As the digital infrastructure is further
elaborated, which entails a greater number of mixed-autonomy vehicles on the
road, the field could move towards the contested quadrant, as logics associated
with safety and liability are disputed. If the rules and regulations to handle
negative externalities brought about by algorithmic errors are not in place,
the field would likely stay in the contested quadrant. </span><span lang=EN-US>As
the advent of AV’s is going to shift the terms of liability </span><span
lang=EN-US>(Marchant &amp; Lindor, 2012)</span><span lang=EN-US>, the scope of
change demands that an entirely new institutional infrastructure has to be
developed and elaborated by insurers, policymakers, legislators, and
automakers, which could take years and be subject to multiple areas of contestation
among stakeholders.</span></p>

<p class=MsoNormal><span lang=EN-US>Another common pathway is the movement from
an established to a contested field condition. This move is likely to occur
through more disruptive change, either an exogenous shock, e.g., new regulation
(coercive isomorphism), or a strong social movement (normative isomorphism), or
through the challenging of status quo by a new or peripheral actor </span><span
lang=EN-US>(Castel &amp; Friedberg, 2010</span><span lang=EN-US>; </span><span
lang=EN-US>Hensmans, 2003)</span><span lang=EN-US>. The use of recommender
engines (RE), which suggests products, services, and other online information
to users based on prior data, is already a well-established practice but could
become contested due to emerging incoherencies in logics. RE’s have, for
example, been argued to create fragmentation by limiting a users’ media
exposure to a set of predefined interests or objectives </span><span
lang=EN-US>(Sunstein, 2007)</span><span lang=EN-US>, which could have
undesirable societal consequences as people’s preferences may be guided towards
echo chambers, where alternate views are missing </span><span
lang=EN-US>(Hosanagar, Fleder, Lee, &amp; Buja, 2014)</span><span lang=EN-US>,
which further </span><span lang=EN-US>impedes decisional autonomy </span><span
lang=EN-US>(Newell &amp; Marabelli, 2015)</span><span lang=EN-US>.</span><span
lang=EN-US> Other actors argue that existing data are inconclusive, and some
research suggests that recommenders appear to create commonality, not
fragmentation </span><span
lang=EN-US>(Van Alstyne &amp; Brynjolfsson, 2005)</span><span lang=EN-US>,
implying that there is little cause to modify the current architecture of
recommender engines </span><span
lang=EN-US>(Hosanagar et al., 2014</span><span lang=EN-US>; </span><span
lang=EN-US>Möller, Trilling, Helberger, &amp; van Es, 2018)</span><span
lang=EN-US>.  This incoherency in logics is coupled with information
asymmetries between the AI-agent and human actors in relation to how, and on
which information, a decision to recommend certain content is rendered. This
lack of transparency, as well as a lack of algorithmic knowledge by the general
population, leaves elements of the current digital infrastructure in the
contested quadrant. The governance of data and information that goes into a
recommender engine, for example, is partially situated in the contested
quadrant, which could have wider field-level implications, and possibly force a
coercive change in the form of new regulation (Sauder, 2008).</span></p>

<p class=MsoNormal><span lang=EN-US>When a field moves from a position of
established to (re)aligning under the emergent quadrant, change is usually
observed through incremental modifications, with central actors often managing these
</span><span
lang=EN-US>(Zietsma et al., 2017)</span><span lang=EN-US>. This incremental
change sees the field realigning around new practices or relational channels
while readjusting the institutional infrastructure. Triggers for this move could
be the introduction of a new technology or law. The field of smart speakers (e.g.,
Google Assistant, Siri, Alexa, etc.) has moved from the emerging towards the
established field-quadrant over a relatively short time-horizon, while certain
elements of the digital infrastructure have been linked to concerns over
data-collection and data privacy practices, which could see the field move to
grow more contested.</span></p>

<p class=MsoNormal><span lang=EN-US>Other pathways of change include a move
from a fragmented or contested condition to one that is aligning in the
emergent quadrant. When looking at nascent AI areas such as Generative
Pre-trained Transformer 3 (GPT-3), or deepfakes, these emerge in the fragmented
quadrant due to incoherent logics, coupled with institutional infrastructures
that are unelaborate. GPT-3 is an autoregressive language model that is able to
produce text that is difficult to distinguish from text written by a human,
whereas deepfakes are synthetic media, for example, an image, video, or voice
that has been algorithmically modified to look or sound authentic, when it is
not. While the inherent agency of these AI systems are emerging, their
associated use of already elaborate digital infrastructure linked to the
general information ecosystem makes them able to proliferate at rapid speeds. In
terms of autonomy, this means that these AI-agents could have a </span><span
lang=EN-US style='color:black'>considerable impact on their environment </span><span
lang=EN-US>by exacerbating the spread of misinformation online.</span><span
lang=EN-US style='color:black'> </span><span lang=EN-US>A move from the
fragmented quadrant towards greater alignment is therefore needed, which may be
formed as actors converge around new ideas, rules, and positions in order to
inform and elaborate the surrounding institutional infrastructure while establishing
greater coherency in logics </span><span
lang=EN-US>(Garud, 2008</span><span lang=EN-US>; </span><span
lang=EN-US>Zilber, 2007)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>AI is currently changing organizational
practices across a wide range of fields, which implies that new applications
should be carefully considered in terms of their short-term impact on human behavior
as well as long-run influences on institutional change. Insufficiently tested
implementation of unsafe or biased algorithms can foster negative externalities,
which can have severe consequences or may be detrimental to societal trust. </span><span
lang=EN-US>An analysis of AI-associated digital institutional infrastructure,
based on logics and work, as well as conceptualizations of technological
maturity, data practices, and AI-autonomy, contributes to assessing where
potential areas of contestation or fragmentation could be found. These findings
hold important implications for AI-developers and adopters (e.g., engineers,
managers, firms), as well as for policymakers that seek to define new rules
going forward. These implications, as well as the main takeaways of the paper,
are briefly discussed below before a conclusion is offered.  </span></p>

<p class=MsoListParagraph style='text-indent:-18.0pt'><b><span lang=EN-US>6.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp; </span></span></b><b><span
lang=EN-US>Discussion: Commonalities of AI-induced Field Change &amp; Pending
Issues over Governance </span></b></p>

<p class=MsoNormal><span lang=EN-US>Through application of the developed framework,
three main takeaways that have implications for wider AI-induced field change
and legitimization have been found. At a general level, these refer to (1) altered
power-dependencies between humans and machines, (2) unresolved questions over data-use
and control, as well as (3) issues with the current elaboration of
institutional infrastructure that surrounds many AI applications. </span></p>

<p class=MsoNormal><span lang=EN-US>First, the autonomy of AI agents can affect
existing power-dependencies, which may cause friction as</span><span
lang=EN-US> human behavior and ways of organization are influenced in ways that
are hard to identify ex-ante </span><span
lang=EN-US>(Curchod et al., 2020)</span><span lang=EN-US>.</span><span
lang=EN-US> </span><span lang=EN-US>In examples such as facial recognition, judicial
AI-systems, autonomous vehicles, and so on, the AI-agent gains determining
power over human actors, which have to trust the identifications or predictions
of the AI-agent. This transfer of autonomy is contingent on systemic trust,
which is based on conceptualizations of technological maturity and ideas of
machine-augmented perception that is expected to operate at cognitive levels
that are equal to – or in many cases exceeds those of a human operator. Issues
with field-level legitimization and nascent processes of institutionalization
are therefore likely to arise when emerging systems are inaccurate, unsafe, or
intransparent, which erode trust across applications and causes fields to stay
fragmented and logics to grow incoherent. Analyzing the field trajectories of
these cases involves assessing what it takes for altered power-dependencies to
be conceived as legitimate practices, which is crucial for a field to move from
fragmentation or contestation towards greater alignment of digital and
institutional infrastructures. </span></p>

<p class=MsoNormal><span lang=EN-US>Second, </span><span lang=EN-US>an
incentive for data-centralization is inherent in most digital infrastructures
(based on technical and economic logics), which has implications for associated
forms of organization. A lack of transparency during the processes of data
collection, as well as in markets for data, are leaving large populations
unaware of where and how their personal data and information is being used,
stored, and traded, as well as for what purposes </span><span
lang=EN-US>(Mittelstadt et al., 2016)</span><span lang=EN-US>. The current organization
of many digital infrastructures thus come with the risk of deteriorating public
trust in digital institutional infrastructures if data-sources are used for
socially disputed measures of public (e.g., safety) and private (e.g., market-based)
forms of surveillance </span><span
lang=EN-US>(Zuboff, 2019)</span><span lang=EN-US>, or are being misused, e.g.,
due to large-scale data-breaches </span><span
lang=EN-US>(Isaak &amp; Hanna, 2018)</span><span lang=EN-US>. This implies that
the legitimacy of AI-agents is highly contingent on legitimate collection, use,
and ownership of data, which otherwise could be a source of dispute that causes
field-level disintegration. </span></p>

<p class=MsoNormal><span lang=EN-US>Regulations such as the European Union’s
General Data Protection Regulation (GDPR) should be seen as the first step of
elaborating institutional infrastructure that seeks to move fields engaged in
data-collection from the contested quadrants towards greater establishment and
coherency in logics. Over time this could imply a conceptual shift of companies
moving from “owners” towards “custodians” of individuals’ private data. Opening
access to data and developing interactivity, as well as an increased sense of
ownership with users, is a step that could gain traction in order to smoothen existing
information asymmetries between central actors and individual end-users </span><span
lang=EN-US>(Tene &amp; Polonetsky, 2013)</span><span lang=EN-US>. Similarly,
empowering users to better understand and perhaps interact with certain
AI-agents (e.g., recommender engines) would empower these with a greater sense
of ownership over how streams of information are utilized and handled, as well
as impacting individual practices and forms of behavior</span></p>

<p class=MsoNormal><span lang=EN-US>Third, where institutional infrastructure
is considered inadequate during phases of market expansion, peripheral actors,
such as civil society organizations, frequently work on outlining insufficient
governance arrangements </span><span
lang=EN-US>(Star, 2002)</span><span lang=EN-US>. In many cases, it is important
that institutional infrastructure is elaborated before negative externalities
start to erode systemic and institutional levels of trust, which causes a field
to grow fragmented. If trust is eroded past certain barriers, technology
developers and adopters are likely to experience severe pushback from the general
public. Public pushback forces central actors from the private sector to engage
in new measures of self-regulation, which in some cases means scaling back
digital infrastructure until a policy-vacuum is filled by new legislative
provisions, such as in the case of facial recognition technologies. When logics
are at odds with existing power structures or violate existing governance
arrangements, these are also more likely to be resisted </span><span
lang=EN-US>(Furnari, 2016)</span><span lang=EN-US>. </span></p>

<p class=MsoNormal><span lang=EN-US>At the same time, institutional
infrastructures must emerge as more adaptive forms of organization that are able
to take into account the myriad ways in which modular AI-systems influence and
shape existing practices, ways of behavior, and forms of organization. This
warrants that new types of institutional engineering have to be embraced in
order to keep up with rapidly expanding digital infrastructures while
alleviating the pacing problem </span><span
lang=EN-US>(Hagemann et al., 2018)</span><span lang=EN-US>. Proposed measures
of institutional adaptation to mitigate AI-induced externalities include
enhanced measures of </span><span lang=EN-US>algorithmic auditing carried out
by companies </span><span
lang=EN-US>(Zarsky, 2016)</span><span lang=EN-US>, third-party auditors </span><span
lang=EN-US>(Clark &amp; Hadfield, 2019)</span><span lang=EN-US>, or external
regulators </span><span
lang=EN-US>(Tutt, 2016)</span><span lang=EN-US>.</span></p>

<p class=MsoNormal><span lang=EN-US>Auditing can create an ex-post procedural
record of complex algorithmic decision-making in order to track inaccurate
decisions or to detect forms of discrimination, as well as biased data, practices,
and other harms </span><span
lang=EN-US>(Mittelstadt et al., 2016)</span><span lang=EN-US>. When algorithms
are designed without considering a population’s or community’s needs, it has become
more apparent that both the algorithm and its implementer are likely to experience
public pushback or outright rejection, which obstructs processes of AI legitimacy
and adoption </span><span
lang=EN-US>(Whittaker et al., 2018)</span><span lang=EN-US>. </span></p>

<p class=MsoNormal><span lang=EN-US>As a growing number of fields continue to
migrate from traditional forms of linear programming and further embrace autonomous
learning algorithms – behavioral control is gradually transferred from the
programmer to the algorithm and its operating environment </span><span
lang=EN-US>(Matthias, 2004)</span><span lang=EN-US>. During this process, “the
modular design of systems can mean that no single person or group can fully
grasp the manner in which the system will interact or respond to a complex flow
of new inputs” </span><span
lang=EN-US>(Allen, Wallach, &amp; Smit, 2006</span><span lang=EN-US>: 14). In
order to cope with AI-induced complexities, </span><span lang=EN-US>new
governance structures have to be co-invented through greater stakeholder
engagement among companies, civil society organizations, as well as
policymakers in order to secure the </span><span lang=EN-US>inclusion of
affected communities in the development of just algorithmic systems and
processes going forward </span><span
lang=EN-US>(Lee et al., 2019)</span><span lang=EN-US>. </span></p>

<p class=MsoNormal><span lang=EN-US>The tradeoffs between algorithmic accuracy,
transparency, and use of data, as well as the rights to privacy, explanation,
and right of redress, remain subject to ongoing forms of mediation in relation
to the concomitant organizational practices that emerge at the intersection of human-machine-based
interactions. While these tradeoffs have wide-ranging implications for the kind
of institutions that are likely to emerge, the devising of inclusive yet
reflexive institutional infrastructures that are able to encompass a wide
variety of AI-associated risks remains a crucial area to be studied for years
to come.</span></p>

<p class=MsoListParagraph style='text-indent:-18.0pt'><b><span lang=EN-US>7.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp; </span></span></b><b><span
lang=EN-US>Conclusion</span></b></p>

<p class=MsoNormal><span lang=EN-US>The increased presence of AI-agents
embedded in varying forms of organization entails that a whole range of AI-induced
institutions are currently emerging. This paper makes three contributions that
help elicit the ways in which AI-induced fields are subject to varying degrees
of legitimacy as well as processes of institutionalization.</span></p>

<p class=MsoNormal><span lang=EN-US>First, the paper develops </span><span
lang=EN-US>a novel framework for analyzing AI-induced field change. Second,
through an illustration of the framework and application of the concept of
pathways of change, a set of common grounds for contestation associated with
AI-induced field change have been found. These relate to </span><span
lang=EN-US>altered power-dependencies between humans and machines, the
importance of data as a source of legitimacy, and the need for more official
rulemaking to guide the expansion of AI applications.</span><span lang=EN-US> At
last, the paper points to </span><span lang=EN-US>the need for more adaptive
organizations to emerge in response to AI-systems.</span></p>

<p class=MsoNormal><span lang=EN-US>The notion </span><span lang=EN-US>of
pathways of change helps elicit the varying ways in which novel AI solutions
are resisted, rejected, or accepted as legitimate practices over time. </span><span
lang=EN-US>Assessing where a field is currently positioned, as well as what its
potential trajectories are, or could be, and what needs to be done for a field
to grow established and become broadly legitimatized over time, are essential
considerations for stakeholders to take into account. Such deliberations
contribute to secure greater alignment between digital and institutional
infrastructures, which is important in terms of mitigating negative
externalities going forward. </span></p>

<p class=MsoNormal><span lang=EN-US>The logics of any algorithmic interaction,
as well as transparency with the information that guides the interaction, needs
to be broadly examined in order to get a better understanding of how a given
AI-agent affects and potentially alters existing dependencies between humans
and machines, as well as between humans and new forms of organization. Only by
understanding where certain negative externalities could potentially arise can
organizations that are responsible for algorithmic development or
implementation work on establishing the necessary institutional infrastructure
(i.e., standards, rules, and processes) to keep such externalities in check.
Thinking about where algorithmic decisions, including their basis for taking
actions, as well as the rationales on which such actions are predicated, can
help avoid fragmentation of logics, which causes practices and eventually
fields to grow unstable.  </span></p>

<p class=MsoNormal><span lang=EN-US>Transparent and reliable AI systems, as
well as enhanced human-AI interactions, is a key element for the trajectory of
most AI fields on their road to secure a broad sense of social legitimacy as
well as growing established over time. The dialectical relationship between
stakeholders from private, public, and civil-society continues to play an
important role in mediating disruptive AI technologies. It takes broad
engagement, transparency as well as greater information-symmetry to build
systemic trust across AI-systems and human-machine based interactions. As novel
digital infrastructures continue to emerge, it is important that their road to
becoming institutionalized structures of society is thoroughly vetted and
mitigated in order to secure fair, equitable, and trustworthy socio-technical
interactions in the years to come.</span></p>

<p class=MsoCommentText><span lang=EN-US style='font-size:11.0pt'>&nbsp;</span></p>

<p class=MsoCommentText><span lang=EN-US style='font-size:11.0pt'>&nbsp;</span></p>

<p class=MsoCommentText><span lang=EN-US style='font-size:11.0pt'>&nbsp;</span></p>

<p class=MsoCommentText><span lang=EN-US style='font-size:11.0pt'>&nbsp;</span></p>

<p class=MsoCommentText><span lang=EN-US style='font-size:11.0pt'>&nbsp;</span></p>

<p class=MsoCommentText><span lang=EN-US style='font-size:11.0pt'>&nbsp;</span></p>

<p class=MsoCommentText><span lang=EN-US style='font-size:11.0pt'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN-US>&nbsp;</span></p>

<p class=MsoNormal><b><span lang=EN-US>Bibliography</span></b></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span
lang=EN-US>(PDPC), P. D. P. C. S., &amp; (IMDA), I. M. D. A. (2020). <i>Model
Artificial Intelligence Governance Framework Second Edition</i>.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Allen, C., Wallach, W., &amp;
Smit, I. (2006). Why machine ethics? <i>Machine Ethics</i>, 51–61.
https://doi.org/10.1017/CBO9780511978036.005</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Amis, J., Slack, T., &amp;
Hinings, C. R. (2004). The Pace, Sequence, and Linearity of Radical Change
Author(s): <i>The Academy of Management Journal</i>, <i>47</i>(1), 15–39.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Amodei, D., Olah, C., Steinhardt,
J., Christiano, P., Schulman, J., &amp; Mané, D. (2016). Concrete Problems in
AI Safety. <i>ArXiv</i>, 1–29. Retrieved from http://arxiv.org/abs/1606.06565</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Arnold, M., Piorkowski, D.,
Reimer, D., Richards, J., Tsay, J., Varshney, K. R., … Olteanu, A. (2019).
FactSheets: Increasing trust in AI services through supplier’s declarations of
conformity. <i>IBM Journal of Research and Development</i>, <i>63</i>(4–5),
1–13. https://doi.org/10.1147/JRD.2019.2942288</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Baskerville, R. L., Myers, M. D.,
&amp; Yoo, Y. (2019). Digital First: The Ontological Reversal and New
Challenges for IS Research. <i>MIS Quarterly</i>.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Beer, D. (2017). The social power
of algorithms. <i>Information Communication and Society</i>, <i>20</i>(1),
1–13. https://doi.org/10.1080/1369118X.2016.1216147</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Bender, E. M., &amp; Friedman, B.
(2018). Data Statements for Natural Language Processing: Toward Mitigating
System Bias and Enabling Better Science. <i>Transactions of the Association for
Computational Linguistics</i>, <i>6</i>, 587–604. https://doi.org/10.1162/tacl_a_00041</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Botzem, S., &amp; Dobusch, L.
(2012). Standardization Cycles : A Process Perspective on the Formation and
Diffusion of Transnational Standards. <i>Organization Studies</i>, <i>5</i>–<i>6</i>(33),
737–762. https://doi.org/10.1177/0170840612443626</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Brynjolfsson, E., &amp; McAfee, A.
(2014). The Second Machine Age: Work, Progress, and Prosperity in a Time of
Brilliant Technologies. <i>Quantitative Finance</i>.
https://doi.org/10.1080/14697688.2014.946440</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Castel, P., &amp; Friedberg, E.
(2010). Institutional change as an interactive process: The case of the
modernization of the French cancer centers. <i>Organization Science</i>, <i>21</i>(2),
311–330. https://doi.org/10.1287/orsc.1090.0442</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Clark, J., &amp; Hadfield, G. K.
(2019). REGULATORY MARKETS FOR AI SAFETY. <i>Conference Paper at ICLR 2019</i>.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Constantinides, P., Henfridsson,
O., &amp; Parker, G. G. (2018). Platforms and infrastructures in the digital
age. <i>Information Systems Research</i>, <i>29</i>(2), 381–400.
https://doi.org/10.1287/isre.2018.0794</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Coyle, D., Diepeveen, S., Wdowin,
J., Kay, L., &amp; Tennison, J. (2020). The Value of Data: Policy Implications.
<i>Bennett Institute for Public Policy, Cambridge in Partnership with the Open
Data Institute.</i></span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Curchod, C., Patriotta, G., Cohen,
L., &amp; Neysen, N. (2020). Working for an Algorithm: Power Asymmetries and
Agency in Online Work Settings. <i>Administrative Science Quarterly</i>, <i>65</i>(3),
644–676. https://doi.org/10.1177/0001839219867024</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Dacin, M. T., Ventresca, M. J.,
&amp; Beal, B. D. (1999). The embeddedness of organizations: Dialogue &amp;
directions. <i>Journal of Management</i>, <i>25</i>(3), 317–356.
https://doi.org/10.1177/014920639902500304</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Davenport, T., &amp; Kalakota, R.
(2019). The potential for artificial intelligence in healthcare. <i>Future
Healthcare Journal</i>, <i>6</i>(2), 94–98.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>DiMaggio, P. J., &amp; Powell, W.
W. (1983). The Iron Cage Revisited : Institutional Isomorphism and Collective
Rationality in Organizational Fields. <i>American Sociological Review</i>, <i>48</i>(2),
147–160.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Djelic, M., &amp;
Sahlin-Andersson, K. (2006). <i>Transnational governance: Institutional
dynamics of regulation</i>. Cambridge: Cambridge University Press.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Fligstein, N. (2001). Social skill
and the theory of fields. <i>Sociological Theory</i>, <i>19</i>(2), 105–125.
https://doi.org/10.1111/0735-2751.00132</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Fligstein, N., &amp; McAdam, D.
(2012). <i>A theory of fields</i>. Oxford, U.K: Oxford University Press.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Floridi, L. (2014). <i>The Fourth
Revolution: How the infosphere is reshaping human reality</i>. Oxford, U.K:
Oxford University Press.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Floridi, L., &amp; Sanders, J. W.
(2004). On the morality of artificial agents Luciano. <i>Minds and Machines</i>.
https://doi.org/10.1023/B</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Furnari, S. (2014). Interstitial
Spaces: Micro-Interaction Settings and the Genesis of New Practices between
Institutional Fields. <i>Academy of Management Review</i>, <i>39</i>(4), 1–55.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Furnari, S. (2016). Institutional
fields as linked arenas: Inter-field resource dependence, institutional work
and institutional change. <i>Human Relations</i>, <i>69</i>(3), 551–580.
https://doi.org/10.1177/0018726715605555</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Garud, R. (2008). Conferences as
venues for the configuration of emerging organizational fields: The case of
cochlear implants. <i>Journal of Management Studies</i>, <i>45</i>(6),
1061–1088. https://doi.org/10.1111/j.1467-6486.2008.00783.x</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Garud, R., Jain, S., &amp;
Kumaraswamy, A. (2002). Institutional entrepreneurship in the sponsorship of
common technological standards: The case of Sun Microsystems and Java. <i>Academy
of Management Journal</i>, <i>45</i>(1), 196–214.
https://doi.org/10.2307/3069292</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Garud, R., &amp; Karnøe, P.
(2003). Bricolage versus breakthrough: distributed and embedded agency in
technology entrepreneurship. <i>Research Policy</i>, <i>32</i>, 277–300.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Gawer, A., &amp; Phillips, N.
(2013). Institutional Work as Logics Shift : The Case of Intel ’ s Transformation
to Platform Leader. <i>Organization Science</i>, <i>34</i>(8), 1035 –1071.
https://doi.org/10.1177/0170840613492071</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Gebru, T., Morgenstern, J.,
Vecchione, B., Vaughan, J. W., Wallanch, H., Daume, H., &amp; Crawford, K.
(2020). Datasheets for Datasets. <i>Arxiv</i>.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Ghaffary, S., &amp; Molla, R.
(2019, December 10). Here’s where the US government is using facial recognition
technology to surveil Americans. <i>Vox</i>. Retrieved from
https://www.vox.com/recode/2019/7/18/20698307/facial-recognition-technology-us-government-fight-for-
the-future</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Greenwood, R., Raynard, M.,
Kodeih, F., Micelotta, E. R., &amp; Lounsbury, M. (2011). Institutional
complexity and organizational responses. <i>Academy of Management Annals</i>, <i>5</i>(1),
317–371. https://doi.org/10.1080/19416520.2011.590299</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Greenwood, R., Suddaby, R., &amp;
Hinings, C. R. (2002). Theorizing change: The role of professional associations
in the transformation of institutionalized fields. <i>Academy of Management
Journal</i>, <i>45</i>(1), 58–80. https://doi.org/10.2307/3069285</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Grigorescu, S., Trasnea, B.,
Cocias, T., &amp; Macesanu, G. (2020). A survey of deep learning techniques for
autonomous driving. <i>Journal of Field Robotics</i>, <i>37</i>(3), 362–386.
https://doi.org/10.1002/rob.21918</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hagemann, R., Huddleston, J.,
&amp; Thierer, A. D. (2018). <i>Soft Law for Hard Problems: The Governance of
Emerging Technologies in an Uncertain Future</i>. <i>Colorado Technology Law
Journal</i> (Vol. 17). Retrieved from
https://heinonline.org/HOL/Page?handle=hein.journals/jtelhtel17&amp;id=52&amp;div=8&amp;collection=journals%0Ahttps://ssrn.com/abstract=3118539</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hao, K. (2020a). IBM’s
photo-scraping scandal shows what a weird bubble AI researchers live in. <i>MIT
Technology Review</i>. Retrieved from
https://www.technologyreview.com/2019/03/15/136593/ibms-photo-scraping-scandal-shows-what-a-weird-bubble-ai-researchers-live-in/</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hao, K. (2020b). The two-year
fight to stop Amazon from selling face recognition to the police. <i>MIT
Technology Review</i>.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Henfridsson, O., Nandhakumar, J.,
Scarbrough, H., &amp; Panourgias, N. (2018). Recombination in the open-ended
value landscape of digital innovation. <i>Information and Organization</i>, <i>28</i>(2),
89–100. https://doi.org/10.1016/j.infoandorg.2018.03.001</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hengstler, M., Enkel, E., &amp;
Duelli, S. (2016). Applied artificial intelligence and trust-The case of
autonomous vehicles and medical assistance devices. <i>Technological
Forecasting and Social Change</i>, <i>105</i>, 105–120.
https://doi.org/10.1016/j.techfore.2015.12.014</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hensmans, M. (2003). Social
movement organizations: A metaphor for strategic actors in institutional
fields. <i>Organization Studies</i>, <i>24</i>(3), 355–381.
https://doi.org/10.1177/0170840603024003908</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hill, K. (2020a). The Secretive
Company That Might End Privacy as We Know It. <i>New York Times</i>. Retrieved
from https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hill, K. (2020b). Wrongfully
Accused by an Algorithm. <i>New York Times</i>. Retrieved from
https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hinings, B., Gegenhuber, T., &amp;
Greenwood, R. (2018). Digital innovation and transformation: An institutional
perspective. <i>Information and Organization</i>, <i>28</i>(1), 52–61.
https://doi.org/10.1016/j.infoandorg.2018.02.004</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hinings, B., Logue, D., &amp;
Zietsma, C. (2017). Fields, Institutional Infrastructure and Governance. In <i>The
SAGE Handbook of Organizational Institutionalism</i> (pp. 163–189).</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Holm, P. (1995). The Dynamics of
Institutionalization: Transformation Processes in Norwegian Fisheries. <i>Administrative
Science Quarterly</i>, <i>40</i>(3), 398–422.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Hosanagar, K., Fleder, D., Lee,
D., &amp; Buja, A. (2014). Will the global village fracture into tribes
recommender systems and their effects on consumer fragmentation. <i>Management
Science</i>, <i>60</i>(4), 805–823. https://doi.org/10.1287/mnsc.2013.1808</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Isaak, J., &amp; Hanna, M. J.
(2018). User Data Privacy: Facebook, Cambridge Analytica, and Privacy
Protection. <i>Computer</i>, <i>51</i>(8), 56–59.
https://doi.org/10.1109/MC.2018.3191268</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Kallinikos, J. (2011). <i>Governing
through Technology: Information Artefacts and Social Practices. New York:
Palgrave Macmillan.</i> New York: Palgrave Macmillan.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Kiran, A. H., Oudshoorn, N., &amp;
Verbeek, P. P. (2015). Beyond checklists: toward an ethical-constructive
technology assessment. <i>Journal of Responsible Innovation</i>, <i>2</i>(1),
5–19. https://doi.org/10.1080/23299460.2014.992769</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Krishna, A. (2020). IBM CEO’s
Letter to Congress on Racial Justice Reform. Retrieved from
https://www.ibm.com/blogs/policy/facial-recognition-sunset-racial-justice-reforms/</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Lash, S. (2007). Power after
Hegemony: Cultural Studies in Mutation? <i>Theory, Culture &amp; Society</i>, <i>24</i>(3),
55–78. https://doi.org/10.1177/0263276407075956</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Lawrence, T. B., &amp; Suddaby, R.
(2006). Institutions and institutional work. In <i>The SAGE Handbook of
Organization Studies</i> (pp. 215–254).
https://doi.org/10.4135/9781848608030.n7</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Lee, M. K. (2018). Understanding
perception of algorithmic decisions: Fairness, trust, and emotion in response
to algorithmic management. <i>Big Data and Society</i>, <i>5</i>(1), 1–16.
https://doi.org/10.1177/2053951718756684</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Lee, M. K., Kusbit, D., Kahng, A.,
Kim, J. T., Yuan, X., Chan, A., … Procaccia, A. D. (2019). Webuildai:
Participatory framework for algorithmic governance. <i>Proceedings of the ACM
on Human-Computer Interaction</i>, <i>3</i>(CSCW).
https://doi.org/10.1145/3359283</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Mackenzie, A. (2006). <i>Cutting
Code: Software and Sociality</i>. New York: Peter Lang Publishing.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Macnish, K. (2012). Unblinking
eyes: The ethics of automating surveillance. <i>Ethics and Information
Technology</i>, <i>14</i>(2), 151–167.
https://doi.org/10.1007/s10676-012-9291-0</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Maguire, S., &amp; Hardy, C.
(2009). Discourse and Deinstitutionalization: The Decline of DDT. <i>The
Academy of Management Journal</i>, <i>52</i>(1), 148–178. Retrieved from
https://doi.org/10.5465/amj.2009.36461993</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Marchant, G. E., &amp; Lindor, R.
A. (2012). The Coming Collision Between Autonomous Vehicles and the Liability
System THE COMING COLLISION BETWEEN AUTONOMOUS VEHICLES AND THE LIABILITY
SYSTEM. <i>Santa Clara Law Review Article</i>, <i>52</i>(4).</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Markus, M. L. (2017).
Datification, Organizational Strategy, and IS Research: What’s the Score? <i>Journal
of Strategic Information Systems</i>, <i>26</i>(3), 233–241. https://doi.org/10.1016/j.jsis.2017.08.003</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Matthias, A. (2004). The
responsibility gap: Ascribing responsibility for the actions of learning
automata. <i>Ethics and Information Technology</i>, <i>6</i>(3), 175–183.
https://doi.org/10.1007/s10676-004-3422-1</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Mckay, C. (2020). Predicting risk
in criminal procedure: actuarial tools, algorithms, AI and judicial
decision-making. <i>Current Issues in Criminal Justice</i>, <i>32</i>(1),
22–39. https://doi.org/10.1080/10345329.2019.1658694</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Meyer, J. W., &amp; Rowan, B.
(1977). Institutionalized Organizations : Formal Structure as Myth and
Ceremony. <i>American Journal of Sociology</i>, <i>83</i>(2), 340–363.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Mitchell, M., Wu, S., Zaldivar,
A., Barnes, P., Vasserman, L., Hutchinson, B., … Gebru, T. (2019). Model cards
for model reporting. <i>FAT* 2019 - Proceedings of the 2019 Conference on
Fairness, Accountability, and Transparency</i>, (Figure 2), 220–229.
https://doi.org/10.1145/3287560.3287596</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Mittelstadt, B. D., Allo, P.,
Taddeo, M., Wachter, S., &amp; Floridi, L. (2016). The ethics of algorithms:
Mapping the debate. <i>Big Data and Society</i>, (December), 1–21.
https://doi.org/10.1177/2053951716679679</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Möller, J., Trilling, D.,
Helberger, N., &amp; van Es, B. (2018). Do not blame it on the algorithm: an
empirical assessment of multiple recommender systems and their impact on
content diversity. <i>Information Communication and Society</i>, <i>21</i>(7),
959–977. https://doi.org/10.1080/1369118X.2018.1444076</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Munir, K. A., &amp; Phillips, N.
(2005). The birth of the “Kodak moment”: Institutional entrepreneurship and the
adoption of new technologies. <i>Organization Studies</i>, <i>26</i>(11),
1665–1687. https://doi.org/10.1177/0170840605056395</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Nambisan, S. (2016). Digital
Entrepreneurship: Toward a Digital Technology Perspective of Entrepreneurship. <i>Entrepreneurship:
Theory and Practice</i>, <i>41</i>(6), 1029–1055. https://doi.org/10.1111/etap.12254</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>National Institute of Standards
and Technology (NIST). (2020). NIST. Retrieved from
https://pages.nist.gov/frvt/html/frvt11.html#overviewhttps://www.technologyreview.com/2019/03/15/13
6593/ibms-photo-scraping-scandal-shows-what-a-weird-bubble-ai-researchers-live-in/</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Newell, S., &amp; Marabelli, M.
(2015). Strategic opportunities (and challenges) of algorithmic
decision-making: A call for action on the long-term societal effects of
“datification.” <i>Journal of Strategic Information Systems</i>, <i>24</i>(1),
3–14. https://doi.org/10.1016/j.jsis.2015.02.001</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>North, D. C. (1990). <i>Institutions,
Institutional Change and Economic Performance</i>. <i>Institutions,
Institutional Change and Economic Performance. North, Douglass C.</i>
https://doi.org/10.1017/CBO9780511606892.012</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Nuno, N., Gomes, A. de, &amp;
Kontschieder, V. (2021). AI Impact Assessment: A Policy Prototyping Experiment,
(January).</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Orlikowski, W. J. (2007).
Sociomaterial practices: Exploring technology at work. <i>Organization Studies</i>,
<i>28</i>(9), 1435–1448. https://doi.org/10.1177/0170840607081138</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Orlikowski, W. J., &amp; Scott, S.
V. (2008). 10 Sociomateriality: Challenging the Separation of Technology, Work
and Organization. <i>The Academy of Management Annals</i>, <i>2</i>(1),
433–474. https://doi.org/10.1080/19416520802211644</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Parasuraman, R., &amp; Manzey, D.
H. (2010). Complacency and bias in human use of automation: An attentional
integration. <i>Human Factors</i>, <i>52</i>(3), 381–410.
https://doi.org/10.1177/0018720810376055</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Patvardhan, S. D., Gioia, D. A.,
&amp; Hamilton, A. L. (2015). Weathering a meta-level identity crisis: Forging
a coherent collective identity for an emerging field. <i>Academy of Management
Journal</i>, <i>58</i>(2), 405–435. https://doi.org/10.5465/amj.2012.1049</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Pisano, G. P., &amp; Teece, D. J.
(2007). How to Capture Value from Innovation: Shaping Intellectual Property and
Industry Architecture. <i>California Management Review</i>.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Powell, W. W., Oberg, A., Korff,
V., Oelberger, C., &amp; Kloos, K. (2017). Institutional analysis in a digital
era: Mechanisms and methods to understand emerging fields. In <i>New Themes in
Institutional Analysis: Topics and Issues from European Research</i> (pp.
305–344). https://doi.org/10.4337/9781784716875.00016</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Powell, W. W., &amp; Sandholtz, K.
W. (2012). Amphibious entrepreneurs and the emergence of organizational forms. <i>Strategic
Entrepreneurship Journal</i>, <i>6</i>(2), 94–115.
https://doi.org/10.1002/sej.1129</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Rai, A., Constantinides, P., &amp;
Sarker, S. (2019). Editor’s comments: Next-Generation Digital Platforms: Toward
Human–AI Hybrids. <i>MIS Quarterly</i>, <i>43</i>(1).</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Rao, H., Morrill, C., &amp; Zald,
M. N. (2000). Power plays: How social movements and collective action create
new organizational forms. <i>Research in Organizational Behavior</i>, <i>22</i>(June),
237–281. https://doi.org/10.1016/s0191-3085(00)22007-8</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Raynard, M. (2016). Deconstructing
complexity: Configurations of institutional complexity and structural
hybridity. <i>Strategic Organization</i>, <i>14</i>(4), 310–335.
https://doi.org/10.1177/1476127016634639</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Reay, T., &amp; Hinings, C. R.
(2005). The recomposition of an organizational field: Health care in Alberta. <i>Organization
Studies</i>, <i>26</i>(3), 351–384. https://doi.org/10.1177/0170840605050872</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Reay, T., &amp; Hinings, C. R.
(2009). Managing the rivalry of competing institutional logics. <i>Organization
Studies</i>, <i>30</i>(6), 629–652. https://doi.org/10.1177/0170840609104803</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Reisman, D., Schultz, J.,
Crawford, K., &amp; Whittaker, M. (2018). Algorithmic impact assessments: A
practical framework for public agency accountability. <i>AI Now Institute</i>,
(April), 22. Retrieved from https://ainowinstitute.org/aiareport2018.pdf</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Russell, S. J., &amp; Norvig, P.
(2010). <i>Artificial Intelligence: A Modern Approach</i>. <i>Artificial
Intelligence</i>. Prentice-Hall. https://doi.org/10.1017/S0269888900007724</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Schlesinger, A., O’Hara, K. P.,
&amp; Taylor, A. S. (2018). Let’s talk about race: Identity, chatbots, and AI. <i>Conference
on Human Factors in Computing Systems - Proceedings</i>, <i>2018</i>-<i>April</i>,
1–14. https://doi.org/10.1145/3173574.3173889</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Scott, R. W. (2014). Institutions
and Organizations, 4th ed.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Selbst, A. D., Boyd, D., Friedler,
S. A., Venkatasubramanian, S., &amp; Vertesi, J. (2019). Fairness and
abstraction in socio-technical systems. In <i>FAT* 2019 - Proceedings of the
2019 Conference on Fairness, Accountability, and Transparency</i> (pp. 59–68).
https://doi.org/10.1145/3287560.3287598</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Star, S. L. (2002). Infrastructure
and ethnographic practice: Working on the fringes. <i>Scandinavian Journal of
Information Systems</i>, <i>14</i>(2), 107–122.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Stoyanovich, J., &amp; Howe, B.
(2019). Nutritional Labels for Data and Models. <i>Data Engineering</i>,
(1926250), 13.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Sunstein, C. R. (2007). <i>Republic.com
2.0</i>. Princeton, NJ: Princeton University Press.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Tene, O., &amp; Polonetsky, J.
(2013). Big Data for All : Privacy and User Control in the Age of Analytics Big
Data for All : Privacy and User Control in the. <i>Northwestern Journal of
Technology and Intellectual Property</i>, <i>11</i>(5).</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Thornton, P. H., &amp; Ocasio, W.
(1999). Institutional logics and the historical contingency of power in
organizations: Executive succession in the higher education publishing
industry, 1958-1990. <i>American Journal of Sociology</i>, <i>105</i>(3),
801–843. https://doi.org/10.1086/210361</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Thornton, P. H., Ocasio, W., &amp;
Lounsbury, M. (2012). Introduction to the Institutional Logics Perspective. In <i>The
Institutional Logics Perspective: A New Approach to Culture, Structure and
Process</i>. https://doi.org/10.1093/acprof</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Tilson, D., Lyytinen, K., &amp;
Sørensen, C. (2010). Digital infrastructures: The missing IS research agenda. <i>Information
Systems Research</i>, <i>21</i>(4), 748–759.
https://doi.org/10.1287/isre.1100.0318</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Timmermans, S., &amp; Epstein, S.
(2010). A world of standards but not a standard world: Toward a sociology of
standards and standardization. <i>Annual Review of Sociology</i>, <i>36</i>,
69–89. https://doi.org/10.1146/annurev.soc.012809.102629</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Tutt, A. (2016). AN FDA FOR
ALGORITHMS. <i>Administrative Law Review</i>, <i>69</i>(1).</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Van Alstyne, M., &amp;
Brynjolfsson, E. (2005). Global village or cyber-balkans? Modeling and
measuring the integration of electronic communities. <i>Management Science</i>,
<i>51</i>(6), 851–868. https://doi.org/10.1287/mnsc.1050.0363</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Varshney, K. R., &amp; Alemzadeh,
H. (2017). On the Safety of Machine Learning: Cyber-Physical Systems, Decision
Sciences, and Data Products. <i>Big Data</i>, <i>5</i>(3), 246–255.
https://doi.org/10.1089/big.2016.0051</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Waddock, S. (2008). Building a new
institutional infrastructure for corporate responsibility. <i>Academy of
Management Perspectives</i>, <i>22</i>(3), 87–108.
https://doi.org/10.5465/AMP.2008.34587997</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Whittaker, M., Crawford, K.,
Dobbe, R., Fried, G., Kaziunas, E., Mathur, V., … Schwartz, O. (2018). AI Now
2018 Report. <i>AI Now</i>, (December), 1–62. Retrieved from
https://ainowinstitute.org/AI_Now_2018_Report.pdf</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Yang, K., Stoyanovich, J., Asudeh,
A., Howe, B., Jagadish, H. V., &amp; Miklau, G. (2018). A nutritional label for
rankings. In <i>Proceedings of the ACM SIGMOD International Conference on
Management of Data</i> (pp. 1773–1776). https://doi.org/10.1145/3183713.3193568</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Yoo, Y., Henfridsson, O., &amp;
Lyytinen, K. (2010). The new organizing logic of digital innovation: An agenda
for information systems research. <i>Information Systems Research</i>, <i>21</i>(4),
724–735. https://doi.org/10.1287/isre.1100.0322</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Zarsky, T. (2016). The Trouble
with Algorithmic Decisions: An Analytic Road Map to Examine Efficiency and
Fairness in Automated and Opaque Decision Making. <i>Science, Technology, &amp;
Human Values</i>, <i>41</i>(1), 118–132.
https://doi.org/10.1177/0162243915605575</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Zhu, H., Yu, B., Halfaker, A.,
&amp; Terveen, L. (2018). Value-Sensitive Algorithm Design. <i>Proceedings of
the ACM on Human-Computer Interaction</i>, <i>2</i>(CSCW), 1–23.
https://doi.org/10.1145/3274463</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Zietsma, C., Groenewegen, P.,
Logue, D., &amp; Hinings, C. R. (Bob). (2017). FIELD OR FIELDS? BUILDING THE
SCAFFOLDING FOR CUMULATION OF RESEARCH ON INSTITUTIONAL FIELDS. <i>Academy of
Management Annals</i>, <i>11</i>(1). https://doi.org/10.1017/CBO9781107415324.004</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Zietsma, C., &amp; Lawrence, T. B.
(2010). Institutional work in the transformation of an organizational field:
The interplay of boundary work and practice work. <i>Administrative Science
Quarterly</i>, <i>55</i>(2), 189–221. https://doi.org/10.2189/asqu.2010.55.2.189</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Zilber, T. B. (2007). Stories and
the discursive dynamics of institutional entrepreneurship: The case of Israeli
high-tech after the bubble. <i>Organization Studies</i>, <i>28</i>(7),
1035–1054. https://doi.org/10.1177/0170840607078113</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Zuboff, S. (1988). <i>In the age
of the smart machine: the future of work and power</i>. New York: Basic Books.</span></p>

<p class=MsoNormal style='margin-left:24.0pt;text-indent:-24.0pt;line-height:
normal;text-autospace:none'><span lang=EN-US>Zuboff, S. (2019). <i>The Age of
Surveillance Capitalism</i>. New York: PublicAffairs.</span></p>

<p class=MsoNormal><span lang=EN-US>&nbsp;</span></p>

</div>

</body>

</html>
