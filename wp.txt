 A Framework for Understanding AI-Induced Field Change: How AI Technologies are Legitimized and Institutionalized(Working Paper – Unfinished Draft – Do Not Cite)Benjamin Cedric LarsenPh.D. Fellow - Copenhagen Business School AbstractArtificial intelligence (AI) systems operate in increasingly diverse areas, from healthcare to facial recognition, the stock market, autonomous vehicles, and so on. While the underlying digital infrastructure of AI systems is developing rapidly, each area of implementation is subject to different degrees and processes of legitimization. By combining elements from institutional theory and information systems-theory, this paper presents a conceptual framework to analyze and understand AI-induced field-change. The framework is illustrated through application to the development and adoption of facial recognition technologies in the United States. The introduction of novel AI-agents into new or existing fields creates a dynamic in which algorithms (re)shape organizations and institutions while existing institutional infrastructures determine the scope and speed at which organizational change is allowed to occur. Where institutional infrastructure and governance arrangements, such as standards, rules, and regulations, are unelaborate, the field can move fast but is also more likely to be contested. The institutional infrastructure surrounding AI-induced fields is generally little elaborated, which could be an obstacle to the broader institutionalization of AI-systems going forward.1. Introduction In recent years, the scope of information technology that complements or augments human actions has expanded rapidly. The logics embedded in AI-systems already operate in diverse areas, such as the stock market (Mackenzie, 2006), mortgage underwriting (Markus, 2017), autonomous vehicles (Hengstler, Enkel, & Duelli, 2016), medical services (Davenport & Kalakota, 2019), the judicial system (Mckay, 2020), and a range of other fields. The action-potentials inherent in most AI systems imply a shift in agency, moving from human actors to AI agents, which in turn has a significant impact on shaping new practices (e.g., across healthcare, agriculture, autonomous vehicles, etc.), and thereby new forms of organization. Novel AI systems and agents are embedded in existing digital infrastructures and operate within an institutional framework that enables or constrains various activities (Baskerville, Myers, & Yoo, 2019). The socio-economic embeddedness of AI systems means that some AI agents may affect and alter existing social practices and ways of organization in swift and transforming ways, while implementation may be subject to varying degrees of legitimacy, depending on the field and area of implementation. During this process, human actions themselves have become subject to informatization where behavior is tracked and data points collected (Kallinikos, 2011; Zuboff, 1988, 2019). Data is derived from social networks and online interactions, facial recognition technologies, driving behavior, apps recording location data, and so on. The wide range of AI implementations motivates the research question of this paper, which seeks to understand how AI-induced fields are subject to varying degrees of legitimacy as well as processes of institutionalization.Views from institutional- and information systems (IS) theory are combined in order to conceptualize how AI fields operate at the meso-level in terms of gaining legitimacy, that is, how AI diffusion is adopted and accepted, or rejected, under varying socio-economic conditions. Elements from information systems theory elaborate on the notion of digital infrastructure (Constantinides, Henfridsson, & Parker, 2018; Henfridsson & Bygstad, 2013; Yoo, Henfridsson, & Lyytinen, 2010), which signifies a range of interconnected technologies (e.g., Internet, Platforms, IoT) that contribute to realize the action potentials of novel AI agents and associated processes of information collection. Institutional theory introduces the concept of fields, which is applied in order to denote distinct areas of AI implementation and organization by a diverse range of actors. Elements from institutional theory, i.e., institutional work (Lawrence & Suddaby, 2006), logics (Thornton, Ocasio, & Lounsbury, 2012; Gawer & Phillips, 2013), and infrastructure (Hinings, Logue, & Zietsma, 2017), are applied in order to conceptualize how processes of AI-induced digitization affects the evolution and governance of organizations (Powell, Oberg, Korff, Oelberger, & Kloos, 2017). Theory surrounding institutional work is applied in order to understand how actors accomplish the social construction of logics (i.e., rules, scripts, schemas, and cultural accounts), which signifies where human actors and AI agents may challenge existing organizational or institutional practices and boundaries, which may result in difficulties associated with legitimization. Adding the institutional perspective is about how “digitally-enabled institutional arrangements emerge and diffuse both through fields and organizations” (Hinings, Gegenhuber, & Greenwood, 2018: 53). The primary focus of the paper is placed on the interplay between existing and new and emerging institutional arrangements, as well as the role of AI in altering ways of organization. In combining views from institutional- and information systems (IS) theory, the paper proposes a novel conceptual framework for analyzing and understanding AI-induced field change. The framework builds on Zietsma et al.’s. (2017) concept of pathways of change, which outlines how a field is likely to move between states from emerging/aligning to fragmented, contested, and established, depending on the coherency in logics and elaboration of institutional infrastructure. The proposed framework adds the notion of digital infrastructure elaborated through the constructs of technological maturity, data, and AI autonomy, which enables an assessment of the impact of AI-systems on existing forms of institutional infrastructure. Where digital and institutional infrastructure is well-elaborated in terms of organizational practices, rules, and processes, the field could be considered established. If a field is emerging or aligning, on the other hand, its digital and institutional infrastructure will be nascent and unelaborate. The developed framework is illustrated through application to the field of facial recognition technologies in the United States.The paper makes three contributions. First, it presents a conceptual framework for analyzing AI-induced field change. Second, through illustration of the framework, three common grounds for contestation are found, which obstruct processes of legitimization associated with AI-induced field change. These relate to: altered power-dependencies between humans and machines; the importance of data as a source of legitimacy, as well as; insufficient forms of governance, i.e., self-regulation, which warrants a shift towards greater elaboration of official institutional infrastructure surrounding AI applications. At last, the paper discusses implications of AI-based institutionalization going forward and highlights the need for more adaptive organizations to emerge in order to keep pace with AI-systems’ transformative impact on existing practices and ways of behavior.  The paper is structured as follows. Section 2 elaborates on institutional theory and the characteristics of digital infrastructure. Section 3 presents a framework for understanding AI-induced field change. Section 4 applies the framework through illustration. Section 5 deliberates on pathways of change referring to how AI-fields become institutionalized, and section 6 discusses obstacles to legitimacy as well as paths forward in terms of governance. Section 7 concludes.2. Institutional Theory and AI-AgentsIn organization theory, the idea of institutional infrastructure reflects understandings of the embeddedness of organizations within fields and the structuration of fields that occurs through interactions and institutional activity amongst actors (Dacin, Ventresca, & Beal, 1999). Over the last few decades, organizational fields have become more dynamic, and boundaries between fields have become more porous due to the introduction of new digital infrastructures, such as the Internet (Powell et al., 2017: 336). Early institutional theory developed the notion that organizations come to resemble each other due to socio-cultural pressures, which provide a source of legitimacy (Meyer & Rowan, 1977). A central process is that of isomorphism, demonstrating that organizations are likely to converge through normative, mimetic, and coercive pressures (DiMaggio & Powell, 1983). Mimetic isomorphism holds that organizational legitimacy is achieved through copying other organizations as well as their technologies and practices. Coercive legitimacy refers to societal legitimacy, which often is achieved through legislation, whereas normative legitimacy can be viewed as the appropriate professional standards as well as social acceptance of new technologies. Socio-cultural beliefs and practices thus play an important role in the adoption of new technologies and innovations, as well as contingent processes of legitimization and organizational change (Hinings et al., 2018).Competing institutions may lie within individual populations that inhabit a field, while fields may be contested by multiple, and often competing, institutional logics (Gawer & Phillips, 2013; Greenwood, Raynard, Kodeih, Micelotta, & Lounsbury, 2011; Reay & Hinings, 2005, 2009; Scott, 2014). Institutional logics describe the “socially constructed, historical patterns of material practices, assumptions, values, beliefs, and rules” of a field  (Thornton & Ocasio, 1999: 804). The institutional logics perspective deals with the interrelationships among individuals, institutions, and organizations, i.e., the actors of a field. Institutional work, on the other hand, emphasizes a conceptual shift towards individuals and organization’s actions as “dependent on cognitive (rather than affective) processes and structures and thus suggests an approach… that focuses on understanding how actors accomplish the social construction of rules, scripts, schemas, and cultural accounts” (Lawrence & Suddaby, 2006: 218).When the two approaches are held together, i.e., logics and interrelationships, and structures and practices, these can be expressed as the institutional infrastructure of a field. Institutional infrastructure is established through adjacent activities such as certifying, assuring, and reporting against principles, codes, and standards, as well as through the formation of new associations and networks among organizations, including official rules and regulations (Waddock, 2008). Institutional infrastructure can be clarified in terms of its degree of elaboration (high, low), as well as coherency in logics (unitary, competing) (Hinings et al., 2017).Novel AI agents operating in varying systems also embody distinct logics and cognitive functions (Floridi & Sanders, 2004). While these functions are defined by human actors (e.g., engineers in a company), AI-agents remain subject to different degrees of autonomy, i.e., they are to some extent able to act independently based on intrinsic flows of information. This implies that AI agents have the autonomy to act on (e.g., judicial evidence, road conditions, etc.), as well as interact with (e.g., speech recognition, chatbots) their environments. This new form of artificial agency confounds the paradox of embedded agency, i.e., how actors are able to change institutions when their actions are conditioned by those same institutions (Holm, 1995), by the implication of an AI’s ability to shape human behavior as well as ways of organization – sometimes simultaneously. In other words, algorithms can affect how we conceptualize the world while modifying socio-political forms of organization (Floridi, 2014).Algorithms can be seen as non-human agents endowed with the ability to evaluate, rank, and reward or punish individuals’ actions and positions based on pre-programmed instructions that shape social relationships (Curchod, Patriotta, Cohen, & Neysen, 2020; Orlikowski & Scott, 2008). Algorithms, however, are oftentimes compressed and hidden, and we do not encounter them in the same way that we encounter traditional rules (Lash, 2007; Beer, 2017). The increasing reliance on algorithms as instruments for the regulation of social relationships, coupled with the obscurity of algorithmic evaluation systems, is evidence of new yet subtle ways of exercising power, which alters existing power-dependencies, e.g., through surveillance, online interaction, and so on (Curchod et al., 2020; Macnish, 2012). Algorithms are therefore implicated in the constitution and reproduction of power asymmetries that regulate individuals’ behaviors and ensure their compliance with predefined standards, which in turn can affect human agency (Curchod et al., 2020). It is difficult, however, to identify ex-ante what the socio-economic effects of scaling an AI-system will be (Henfridsson et al., 2018; Rai, Constantinides, & Sarker, 2019), which warrants that extensive experimentation through application may be necessary before AI-based technological diffusion and legitimization are likely to take place. Institutional logics and institutional work provide a foundation to understand the rationalities and practices of actors that implement novel AI-agents, as well as the AI-agents’ systemic impact on their surroundings through their socio-economic embeddedness. An analysis of AI-agents predicated on institutional work and logics can be placed both at the micro-level, seeking to understand the impact of individual AI-agents on specific socio-economic practices, as well as at the meso-level, seeking to understand how actors influence the legitimacy of AI applications in a field. That is, how AI diffusion is adopted and accepted, or rejected, under varying socio-economic and technological conditions. 2.1 Digital InfrastructureDigital infrastructure is made from a multitude of digital building blocks and is defined as the computing and network resources that allow multiple stakeholders to orchestrate their service and content needs (Constantinides, Henfridsson, & Parker 2018). Digital infrastructures are distinct from traditional infrastructures because of their ability to collect, store, and make digital data available across a large number of systems and devices simultaneously (Constantinides et al., 2018). [0]Examples of digital infrastructures include the Internet (Hanseth and Lyytinen 2010, Monteiro 1998); data centers; open standards, e.g., IEEE 802.11 (Wi-Fi), as well as consumer devices such as smartphones.Henfridsson et al. (2018: 90) refer to “digital resources” as entities that serve as building blocks in the creation and capture of value from information. While AI technologies are assembled as digital building blocks, a distinction needs to be made between traditional software systems (i.e., ERP, CRM, WordPress, etc.) and novel AI-systems (computer vision, machine learning, etc.). This distinction is important as a new kind of embedded agency is inherent in most AI systems, which render these as “organizers,” “predictors,” or “controllers” of data flows that are captured by digital infrastructures (Russell & Norvig, 2010).Most digital building blocks are made accessible through online platforms or are proprietarily assembled through open-source code. Digital building blocks are transformational due to the innovative patterns that can be established through “use-recombination” (Henfridsson et al., 2018), while there needs to be separate legitimacy for each building block, as well as collective legitimacy for a new institutional arrangement to emerge (Hinings et al., 2018). It may, for example, be that a platform-based building block holds legitimacy (e.g., a cloud-based AI facial recognition-system) because it performs within a predefined level of accuracy. However, for the organizational or wider institutional arrangement to gain legitimacy, the embeddedness of the building block into a socio-economic system needs to be accepted at a much broader level of implementation. As digital building blocks are created by engineers, and as humans are subject to bias (Parasuraman & Manzey, 2010), this means that the values of the designer can be “frozen into the code, effectively institutionalizing those values’’ (Macnish, 2012: 158). Friedman and Nissenbaum (1996) argue that bias in computer systems can arise in three distinct ways, referring to (1) pre-existing social values found in the ‘‘social institutions, practices and attitudes’’ from which a technology emerges, (2) technical constraints, and (3) emergent aspects that arise through usage, which only can be known ex-post. The distinction between social and technical bias has also been referred to as normative and epistemic concerns (Mittelstadt et al., 2016) or structural and functional risks (Nuno et al., 2021). Functional risks refer to technical areas such as the design and operation of an AI system, including datasets, bias, and performance issues, whereas structural risks refer to the ethical implications of an AI system, including the societal effects of automated decisions.Based on the distinction between functional (i.e., technical) and structural (i.e., social) risks, three analytical constructs, referring to technological maturity, data, and AI-autonomy, are proposed below in order to signify the relative elaboration of AI-associated digital infrastructure.  2.2 Technological Maturity, Data, & AI AutonomyTechnological MaturityAI systems are subject to different degrees of maturity, both in terms of the accuracy of the system (Zhu, Yu, Halfaker, & Terveen, 2018), as well as the elaboration of adjacent technological standards (Garud, Jain, & Kumaraswamy, 2002). The accuracy of an AI-model refers to whether it operates within a predefined acceptable level of performance. In the case of autonomous vehicle safety, for instance, an AI-controller is expected to hold the ability to locate persons and objects from a distance of 100 meters with an accuracy of +/- 20 cm, within a false negative rate of 1% and false-positive rate of 5% (Grigorescu, Trasnea, Cocias, & Macesanu, 2020). In some areas that involve high-stakes decisions (e.g., autonomous driving, credit applications, judicial decisions, and medical recommendations), high accuracy alone may not be sufficient, as these applications require greater trust in AI services (Arnold et al., 2019). In high-risk areas, it is important that the functional aspects of a model (i.e., accuracy, data, etc.) are further elaborated through measures such as certification, testing, auditing, as well as the elaboration of technological standards, which refers to the institutional infrastructure of a field. Depending on the context and the area of use, a range of quantitative measures can be used to evaluate the technological maturity of an AI-induced field. Some suggestions include the measures of scientific output, e.g., research papers, citations, and the intellectual property rights that surround a given field. Important questions relate to whether emerging algorithmic capabilities are under development and going through stages of testing or already are being widely deployed by a small or a large number of actors. For structural implications, it is important to ask questions such as: how does the technological maturity and elaboration (of immature/mature) AI-induced digital infrastructures affect a field? For example, the implementation of chatbots, which may have performed with sufficient accuracy under test environments, have proved to display racial biases and prejudices, as the algorithm continues to learn during actual implementation, which aggravates social harm for certain groups of the population (Schlesinger, O’Hara, & Taylor, 2018). The elements that are used to evaluate and decide whether an AI-system is mature or immature are therefore dependent on its context of implementation, which renders technical aspects alone insufficient when assessing the technological maturity of AI-models and associated digital infrastructure. Several methods have been proposed to evaluate predictive models, such as “model cards for model reporting” (Mitchell et al., 2019), “nutrition labels for rankings” (Yang et al., 2018), “algorithmic impact assessment” forms (Reisman, Schultz, Crawford, & Whittaker, 2018), as well as “fact sheets” (Arnold et al., 2019). These frameworks help organizations establish new organizational practices that characterize model-specifications in more coherent ways while paying special attention to attributes such as accuracy, bias, consistency, transparency, interpretability, and fairness, among others. At a general level, when dominant standards are in place, and the accuracy of an AI-system is deemed safe, reliable, and trustworthy, digital infrastructure is considered elaborate, and higher field legitimacy is expected. If a technology is considered immature, inaccurate, or insufficiently tested, the surrounding digital infrastructure would be considered unelaborate. DataThe nature of the data that feeds into any AI-model or system is also of particular importance, and data can be classified as being either sensitive (e.g., health-related) or non-sensitive (e.g., weather-related), and the nature of the data can be private (i.e., individual data) or public (common/pooled data) (Coyle, Diepeveen, Wdowin, Kay, & Tennison, 2020). Data can also be biased, which makes AI systems prone to inherit either individually coded forms of bias or biases that result from historical or cultural practices, which are reflected in the training data, and could be adopted by the algorithm (Barocas & Selbst, 2014; Diakopoulos, 2015). For an algorithm to be effective, its training data must be representative of the communities that it impacts. The use of digital infrastructures by individuals, machines, and communities, requires institutions to negotiate how bits containing varying kinds of information legitimately can be utilized and (re)arranged by organizations.Several methods have been proposed to evaluate data as well as machine learning models under a variety of conditions. For data, these include “data statements” (Bender & Friedman, 2018), “datasheets for data sets” (Gebru et al., 2020), and “nutrition labels for data sets” (Stoyanovich & Howe, 2019), which seek to evaluate the data that goes into a model across training, testing, and post-implementation scenarios. Sound data practices that are transparent, well-documented, and privacy-preserving, are generally associated with a more elaborate digital infrastructure. Data practices that are biased, undocumented, or otherwise disputed could be considered a sign of unelaborate digital infrastructure.AutonomyAI-agents hold varying degrees of autonomy to act, while the (explorative) actions of an autonomous learning agent may not always be known and can be subject to change depending on the data that is fed into the model (Amodei et al., 2016). An AI-agent can have limited or extensive autonomy to make decisions, while the decisions of an AI agent can have a lenient (e.g., recommender engine, smart speaker) or a severe (e.g., autonomous vehicle, incarceration system, facial recognition) impact on individuals as well as its surroundings, if the algorithm is inaccurate, fails, or is otherwise at fault. This could include aspects such as excessive collection of data or unwilling intrusion of privacy in the case of facial recognition systems, for example. The categorization of an agent’s autonomy, therefore, includes its ability to act, as well as the possible ramifications of its actions. The perceived risk of an AI agent can be understood as the probability that a disruptive event occurs, multiplied by the severity of potential harm to an individual or form of organization (Nuno et al., 2021). The definition of “harm” and the computation of probability and severity is context-dependent and varies across sectors. For instance, the impact of an autonomous decision in medical diagnosis or in autonomous vehicles would, arguably, be greater than that of a product recommendation system ((PDPC) & (IMDA), 2020). Relevant questions include: what risks may be present in model usage (?), as well identification of the potential recipients, likelihood, and magnitude of harms (Amodei et al., 2016). Where risks are taken into consideration and are sufficiently mitigated in relation to avoiding any potential harms, the digital infrastructure could be considered elaborate.The elaboration of AI-associated digital infrastructure across the constructs of technological maturity, data, and AI-autonomy, remain subject to both qualitative and quantitative judgments and measures, which are field-dependent and linked to idiosyncrasies across functional (technical) as well as structural (ethical) risks and considerations. 2.3 GovernanceSince field-level advancements in AI are context-dependent, this means that the existing institutional infrastructure and logics negotiates the actual impact that a technology is allowed to have within a given social context, which differs across geographies. In other words, the flexibility of a digital infrastructure is often restricted by socio-technical and regulatory arrangements (e.g., restrictions on autonomous vehicles, regulations on the use of patient’s medical data, etc.). Oftentimes, layered and interoperable standards and common definitions of application and service interfaces guide the use and growth of digital infrastructures (Tilson, Lyytinen, & Sørensen, 2010) and are necessary for digital infrastructures wider process of institutionalization. As large technology companies usually are the leading innovators of a field, these also carry a crucial weight in the direction of new technology standards (Pisano & Teece, 2007), which generally affects how an industry or a field continues to evolve. Typically, private actors orchestrate digital infrastructures, which brings issues to the forefront, such as the challenge of establishing a governance system, reproducing social order, and incorporating aspects of value appropriation and control (Botzem & Dobusch, 2012; Djelic & Sahlin-Andersson, 2006; Garud et al., 2002; Garud & Karnøe, 2003; Raynard, 2016) The process that renders digital infrastructures institutional occurs when innovators infuse specific norms, values, logics, as well as forms of governance and technological control into the infrastructure, and as the infrastructure becomes more widely adopted and legitimized over time (Gawer & Phillips, 2013; Orlikowski, 2007; Orlikowski & Scott, 2008). Digital institutional infrastructure can thus be viewed as the integration of digital infrastructure and institutional infrastructure, which is defined as standard-setting digital (AI) technologies that enable, constrain and coordinate numerous actors’ actions and interactions in ecosystems, fields, or industries (Hinings et al., 2018; Nambisan, 2016; Star, 2002; Tilson et al., 2010; Timmermans & Epstein, 2010; Yoo et al., 2010). Digital infrastructures, however, tend to emerge more rapidly than institutional infrastructures (e.g., laws and regulations), which is commonly referred to as the pacing problem (Hagemann, Huddleston, & Thierer, 2018). This may create extensive issues if negative externalities are associated with fast-moving technological implementation that is at odds with existing structures or norms for certain actors or groups of a population. 3. A Conceptual Framework for Understanding AI-Induced Field Change By integrating the insights from institutional theory (work, logics) with information systems theory (digital infrastructure), a novel framework for analyzing AI-induced field change is proposed (Table 1). The framework builds on Zietsma et al.’s (2017) conceptualization of pathways of change, which hypothesizes how actors drive change across different sets of field circumstances. The proposed framework extends Zietsma et al.’s (2017) work through incorporating the notion of AI-associated digital infrastructures, which has implications for the structure and organization of (digital) institutions going forward.The proposed framework first considers varying actors and their position in a field before elaborating on these ability to affect the direction of a field, either through the introduction of a new technology, regulation, or a social movement, for example. Next, the relationship among actors as well as their coherency in terms of logics is considered. When logics are unitary, greater field alignment is expected, whereas competing logics means that a field is unsettled. The elaboration of institutional infrastructure is considered by looking at the practices and actions of individual actors as well as organizations in terms of creating, maintaining, and disrupting institutions over time. The notion of field structuring events is particularly important, both in terms of logic formation or disruption, as well as for the elaboration of the institutional infrastructure of a field. The AI-associated digital infrastructure of a field is signified by the proposed constructs of technological maturity, data-specification, and the relative autonomy of an AI-system. Technological maturity refers to the perceived accuracy of an AI agent, as well as the elaboration of areas pertaining to standards, research, IP, etc. The data linked to a model is another important source of institutional legitimacy, both functionally (e.g., non-biased data) as well as structurally (e.g., how an organization is engaged in practices of data collection and usage). Autonomy refers to the relative impact of an AI agent on its general environment, as well as its potentials for exacerbating structural risks and create harm. At last, the governance of a field, as well as the mechanisms that guide algorithmic implementation, are considered. Table 1: Framework for Analyzing AI-Induced Field Change and Legitimization: ACTORS* Subject position: central, middle status, and peripheral actors* Characterized by roles or functions, i.e., field-structuring or governing organizations, formal governance units, field coordinators, etc.DIGITAL INSTITUTIONAL INFRASTRUCTUREStandard-setting digital technologies that enable, constrain, and coordinate numerous actors’ actions and interactions in ecosystems, fields, or industries (Hinings et al., 2018). 



INSTITUTIONAL INFRASTRUCTUREDIGITAL INFRASTRUCTUREEstablished through activities such as: certifying, assuring, and reporting against principles, codes, rules, and standards, as well as through the formation of new associations and networks among organizations, including official rules and regulations (Waddock, 2008).  * Logics: what are the relationships among individuals and organizations in the field? Are logics competing or unitary? Are they based on market, social, or other rationalities?

* Work: what are some of the practices and actions of individuals and organizations that have implications for creating, maintaining, and disrupting institutions over time? 

What effect does institutional change have on hierarchies of status and influence and subsequent power relations? What are some of the field structuring events? Established from a multitude of digital building blocks, defined as the computing and network resources that allow multiple stakeholders to orchestrate their service and content needs (Constantinides et al., 2018). * Technological Maturity: refers to the elaboration of hardware and software-based infrastructures and associated technological standards. Includes the perceived accuracy, safety, and reliability of an AI system/agent.* Data: is the data utilized sensitive or non-sensitive? Is it private or publicly available? Is it centralized, and who has ownership? * Autonomy: refers to whether the AI-agent holds limited or extensive autonomy to act and whether the agent’s actions have a negligible or a considerable impact on its environment and surroundings. 



GOVERNANCE* Combinations of public and private, formal and informal systems that exercise control within a field. * Units and processes that ensure compliance with rules and facilitate ‘the overall smooth functioning and reproduction of the system (e.g., standards, regulations, reward systems, and social control agents that monitor and enforce these).* Governance can differ within and between fields, as well as across geographies, e.g., countries.Based on coherency in logics (unitary, competing)(Hinings et al., 2017), and the elaboration of institutional infrastructure (high, low)(Greenwood et al., 2011), a four-fold classification of field conditions is produced around whether there are settled or unsettled logic prioritizations and limited or elaborated digital and institutional infrastructure (Figure 1)(Zietsma et al., 2017).Where digital and institutional infrastructure is highly elaborate, and there is a unitary dominant logic within the field, the field can be described as established and relatively stable, i.e., the institutional infrastructure is coherent (Zietsma et al., 2017). Formal governance and informal infrastructure elements are elaborate and likely to reinforce each other, leading to a coherent sense of what is legitimate or not within the organizational field (Greenwood, Suddaby, & Hinings, 2002; Zietsma & Lawrence, 2010).  In fields where there is highly elaborate institutional infrastructure but competing logics (low coherency), there could be multiple formal governance and digital and institutional infrastructure arrangements (Zietsma et al., 2017). These arrangements may be in conflict with one another or compete for dominance, which makes the field contested (Reay & Hinings, 2005; Rao, Morrill, & Zald, 2000). Contested refers both to competing digital infrastructures (e.g., technological standards, varying models, and levels of algorithmic accuracy), as well as to stakeholders opposing views. Figure 1: Digital / Institutional Infrastructure and Logics: Framework for Field-Level Change (modified from Zietsma et al. (2017).Fields with low coherency and limited elaboration of digital and institutional infrastructure are described as fragmented, with competing conceptions of what is legitimate. Fields may be fragmented if they emerge in intermediate positions (e.g., biotechnology), which draws on logics and practices from diverse but neighboring fields (Powell & Sandholtz, 2012). A field may also be fragmented as new actors enter an existing field with innovative ideas and designs about products, courses of action, behaviors, as well as new structures and ways of organization (Patvardhan, Gioia, & Hamilton, 2015). In the field of facial recognition technology, for instance, there are multiple competing logics that move across varying stakeholders and demonstrate incoherent views over technological accuracy, as well as the technology’s inherent ability to enhance public safety. Many differing views paired with a limited (but expanding) digital infrastructure situates the field in the fragmented quadrant.When infrastructure has a low degree of elaboration but a high degree of coherency in terms of unitary logics, the field is described as emerging or aligning (Hinings et al., 2017). While the lack of digital and institutional infrastructure in an emerging field may create considerable room for experimentation and change, it may also limit field members’ ability to define and acquire legitimacy and thus contributes to ambiguity, and potentially, the need to draw on ill-suited infrastructure from adjacent fields. One example could be the emergence of autonomous vehicles, drawing on existing legal frameworks in terms of liability, which, however, are ill-suited in terms of covering the accompanying change in agency and responsibility.  Categorizing a field’s present condition as well as its potential trajectories enables us to get a deeper understanding of possible areas of contestation, fragmentation, or alignment, as well as what it takes for an AI-induced field to grow established over time. Before these conditions are further discussed in section 5, the following section applies the developed framework (Table 1) to the field of facial recognition technologies in the United States. The application briefly illustrates the utility of the framework in terms of assessing field-elaboration, while future studies may apply the framework to analyze case-studies at greater length. 4. Analyzing AI-Induced Field Change and Legitimization: Facial Recognition Technology4.1 ActorsThe proliferation of facial recognition technologies in the United States has been supported by large technology companies, which are the central actors of the field (e.g., Apple, Amazon, Google, Microsoft, IBM). While these companies provide their own applications directly to the market, they also modularize facial-recognition technologies and make them accessible for complementors on their platforms. This makes them field structuring organizations since the modularization of FRT-systems embodies best-practices and de-facto industry standards, which other companies align with. Central actors include adopters of FRT-systems, while many of these are U.S public sector agencies. Contractors that specialize in delivering FRT-technology to law-enforcement agencies, as well as the National Institute of Standards and Technology (NIST), hold intermediate positions. Peripheral actors include multistakeholder organizations such as the Partnership on AI, non-profit research organizations such as the Center for Data and Society, as well as research institutes such as The AI Now Institute (NYU). These actors affect the field through public reports and commentaries, paying special attention to issues of technological implementation and social ramifications. Peripheral actors also include opponents of FRT-systems, both in the form of activists, as well as civil society organizations such as The American Civil Liberties Union (ACLU). 4.2 LogicsThe dominant logics behind FRT’s has been driven by private sector companies focused on gaining market share. The logics behind adoption is motivated by enhancing measures of public safety, e.g., in terms of identifying criminals, screening travelers, and processing border immigration. Both logics are highly contested by peripheral actors (e.g., company activists and civil rights organizations)(Hao, 2020b), citing that inaccurate technologies hold the potential of exacerbating racial and social biases and inequities. This signifies that emergent dominant logics are at odds with existing social arrangements, including structures of power and governance, which makes the technology heavily resisted (Furnari, 2016). 4.3 Work: Field Structuring Events In 2019, the local government of San Francisco became the first city in the United States to ban the use of FRT’s by local agencies. In the spring of 2020, nationwide protests against police brutality and racial profiling caused several central actors (IBM, Amazon, Microsoft) to stop providing FRT-technologies to law enforcement agencies altogether. IBM called for “a national dialogue on whether and how facial recognition technology should be deployed by domestic law enforcement agencies” (Krishna, 2020, p.1), and Amazon announced a one-year moratorium on police use of its facial recognition technology, giving policymakers time to set appropriate rules around the use of the technology. Microsoft declared that it would not sell FRT-technology to police departments in the United States until a federal law that regulates the technology is formulated. These actions by some of the central actors in the field signal that the existing institutional infrastructure remains inadequate in terms of governing and addressing the current expansion of FRT-related digital infrastructure. This indicates that even as central actors on the procurement side include many public sector agencies, the necessary institutional infrastructure to guide potential ramifications of immature technological adoption has not yet been formulated. Greater alignment between stakeholders across industry, government, and civil society, is currently needed in order to secure ongoing legitimacy as well as greater field-level elaboration and use of facial recognition technologies.  4.4 Technological Maturity In terms of technological maturity, verification algorithms have achieved accuracy scores of up to 99.97% on standard assessments like the National Institute for Standards and Technology (NIST) Facial Recognition Vendor Test (NIST, 2020). For identification-systems, error rates tend to climb when high-quality images are replaced with the feed from live cameras that normally are utilized in public spaces. Aging is another factor that affects error rates, while accuracies of FRT-systems differ considerably across gender and race. The context, i.e., the specific area of implementation and use, can therefore be said to have wide-reaching consequences for the accuracy-rates of individual FRT-systems.4.5 Data Issues of legitimacy are also inherent in relation to the kinds of data that are being used for training FRT-algorithms. Many databases rely on publicly available face-annotated data, which in some cases are scraped directly from social media platforms and have raised issues over privacy and consent (Hao, 2020a). The company Clearview has, for example, assembled a database containing some 3 billion images, where many have been scraped from public-facing social media platforms (Hill, 2020a). This raises concerns about the legitimacy of data rights and usage, as well as the ability of existing institutional infrastructure to provide, and safeguard, associated rights. The quantity of data is in many cases important for algorithmic training, as well as for retaining levels of accuracy post-deployment, which means that there is an inherent incentive for private, as well as for public-sector adopters, to amass rich databases (e.g., new biometric data), in order to increase and continue to ensure the accuracy of a given system. In several states (e.g., Texas, Florida, Illinois), the FBI is allowed to use facial recognition technology to scan through the Department of Motor Vehicles (DMV) database of drivers’ license photos (Ghaffary & Molla, 2019) in order to generate a more coherent centralized biometric database. As these kinds of data contain personal information, they are classified as being sensitive and vulnerable, both in terms of misuse as well as in relation to cybersecurity breaches and possible identity theft (Coyle et al., 2020).4.6 Autonomy AI in facial recognition-systems is perceived as a new kind of social control agent, which may exert autonomy over law-enforcement officers in relation to issuing arrest orders. If the accuracy of a system is flawed, an officers’ actions are likely to cause social harm whenever an innocent citizen is arrested (Hill, 2020b). The adoption of facial recognition systems for use in law enforcement alters existing power dependencies, as officers have to trust in, and act on, the information that is rendered to them by the system. Facial recognition systems are thus shaping entirely new practices and forms of organization in which the autonomy of the AI-agent is dependent on the delivery of accurate information, which could reinforce a drive towards data-centralization. 4.7 GovernanceThe field of facial recognition technology is fragmented and exhibits low coherency and limited elaboration in terms of institutional infrastructure. A lack of governance is most readily seen in the absence of coherent rules and regulations, while the field is currently going through a shift from self-regulation towards more formalized governance arrangements. This shift has been called for by peripheral actors, and more recently also by central actors from the private sector, which demands new rules to guide legitimate implementation going forward. The case of facial recognition technologies used by law-enforcement highlights the critical role of culture and politics involved in the organization of markets and in creating the governing ‘rules of the game’ (North, 1990; Fligstein, 2001; Fligstein & McAdam, 2013) 5. Pathways of Change: How AI-Fields Move and Gain Legitimacy Field-level change is brought about by a number of distinct triggers such as competition, and as new technologies, business models, or organizational structures are introduced, which change existing relations as well as practices (mimetic isomorphism) (Furnari, 2014; Garud, 2008; Munir & Phillips, 2005). Pathways of change suggest that there are some commonalities to how fields are likely to evolve and where obstacles to legitimization and institutionalization may be found. In order to understand how fields move between states, we need to pay special attention to the scope of change (i.e., which elements change and how much changes) (Maguire & Hardy, 2009), as well as the pace of change (i.e., the speed at which a field moves from one condition to another)(Amis, Slack, & Hinings, 2004).In the case of facial recognition technologies, the field is currently moving from the fragmented towards the contested quadrant, as the number of use-cases (e.g., public surveillance, airport check-ins, smartphones, doorbells, etc.) continues to expand, based on rising technological maturity (e.g., accuracy, standards). While digital infrastructures are expanding, the field continues to be represented by incoherent logics and sparse institutional infrastructure, however. For example, verification-based FRT’s (e.g., unlocking a smartphone) is already a well-established practice and exhibits legitimate institutionalized functions. Identification-based FRT’s (e.g., public surveillance), on the other hand, are more likely to stay contested due to having a lower degree of algorithmic accuracy, which is paired with more severe social impacts linked to the autonomy of AI-agents, and how these alter existing power structures. In order for the field, as a whole, to grow more established, a shift from self-regulation towards formalized governance arrangements and greater alignment and coherency in terms of logics is needed (lack of normative isomorphism). In more authoritarian settings, such as in China, the field of facial recognition is already on its way to becoming established. This signifies that a country’s socio-political setting informs its institutional infrastructure, which has important implications for a technology’s path towards legitimization as well as processes of institutionalization. A pathway that moves from an aligning or emerging field condition to an established condition usually involves a process of convergence, which is commonly observed in the institutionalization of most fields (see, e.g., Munir & Phillips, 2005). The field of autonomous vehicles (AV) is characterized by its emerging digital and institutional infrastructure, which has a low degree of elaboration but some coherency in terms of logics. While the field is currently aligning at a relatively slow pace, it develops in extension of an existing field, which has been elaborated over decades. Large parts of the existing infrastructure are challenged, however, through the introduction of novel AI-agents and a transfer in autonomy from humans to machines. As the digital infrastructure is further elaborated, which entails a greater number of mixed-autonomy vehicles on the road, the field could move towards the contested quadrant, as logics associated with safety and liability are disputed. If the rules and regulations to handle negative externalities brought about by algorithmic errors are not in place, the field would likely stay in the contested quadrant. As the advent of AV’s is going to shift the terms of liability (Marchant & Lindor, 2012), the scope of change demands that an entirely new institutional infrastructure has to be developed and elaborated by insurers, policymakers, legislators, and automakers, which could take years and be subject to multiple areas of contestation among stakeholders.Another common pathway is the movement from an established to a contested field condition. This move is likely to occur through more disruptive change, either an exogenous shock, e.g., new regulation (coercive isomorphism), or a strong social movement (normative isomorphism), or through the challenging of status quo by a new or peripheral actor (Castel & Friedberg, 2010; Hensmans, 2003). The use of recommender engines (RE), which suggests products, services, and other online information to users based on prior data, is already a well-established practice but could become contested due to emerging incoherencies in logics. RE’s have, for example, been argued to create fragmentation by limiting a users’ media exposure to a set of predefined interests or objectives (Sunstein, 2007), which could have undesirable societal consequences as people’s preferences may be guided towards echo chambers, where alternate views are missing (Hosanagar, Fleder, Lee, & Buja, 2014), which further impedes decisional autonomy (Newell & Marabelli, 2015). Other actors argue that existing data are inconclusive, and some research suggests that recommenders appear to create commonality, not fragmentation (Van Alstyne & Brynjolfsson, 2005), implying that there is little cause to modify the current architecture of recommender engines (Hosanagar et al., 2014; Möller, Trilling, Helberger, & van Es, 2018).  This incoherency in logics is coupled with information asymmetries between the AI-agent and human actors in relation to how, and on which information, a decision to recommend certain content is rendered. This lack of transparency, as well as a lack of algorithmic knowledge by the general population, leaves elements of the current digital infrastructure in the contested quadrant. The governance of data and information that goes into a recommender engine, for example, is partially situated in the contested quadrant, which could have wider field-level implications, and possibly force a coercive change in the form of new regulation (Sauder, 2008).When a field moves from a position of established to (re)aligning under the emergent quadrant, change is usually observed through incremental modifications, with central actors often managing these (Zietsma et al., 2017). This incremental change sees the field realigning around new practices or relational channels while readjusting the institutional infrastructure. Triggers for this move could be the introduction of a new technology or law. The field of smart speakers (e.g., Google Assistant, Siri, Alexa, etc.) has moved from the emerging towards the established field-quadrant over a relatively short time-horizon, while certain elements of the digital infrastructure have been linked to concerns over data-collection and data privacy practices, which could see the field move to grow more contested.Other pathways of change include a move from a fragmented or contested condition to one that is aligning in the emergent quadrant. When looking at nascent AI areas such as Generative Pre-trained Transformer 3 (GPT-3), or deepfakes, these emerge in the fragmented quadrant due to incoherent logics, coupled with institutional infrastructures that are unelaborate. GPT-3 is an autoregressive language model that is able to produce text that is difficult to distinguish from text written by a human, whereas deepfakes are synthetic media, for example, an image, video, or voice that has been algorithmically modified to look or sound authentic, when it is not. While the inherent agency of these AI systems are emerging, their associated use of already elaborate digital infrastructure linked to the general information ecosystem makes them able to proliferate at rapid speeds. In terms of autonomy, this means that these AI-agents could have a considerable impact on their environment by exacerbating the spread of misinformation online. A move from the fragmented quadrant towards greater alignment is therefore needed, which may be formed as actors converge around new ideas, rules, and positions in order to inform and elaborate the surrounding institutional infrastructure while establishing greater coherency in logics (Garud, 2008; Zilber, 2007).AI is currently changing organizational practices across a wide range of fields, which implies that new applications should be carefully considered in terms of their short-term impact on human behavior as well as long-run influences on institutional change. Insufficiently tested implementation of unsafe or biased algorithms can foster negative externalities, which can have severe consequences or may be detrimental to societal trust. An analysis of AI-associated digital institutional infrastructure, based on logics and work, as well as conceptualizations of technological maturity, data practices, and AI-autonomy, contributes to assessing where potential areas of contestation or fragmentation could be found. These findings hold important implications for AI-developers and adopters (e.g., engineers, managers, firms), as well as for policymakers that seek to define new rules going forward. These implications, as well as the main takeaways of the paper, are briefly discussed below before a conclusion is offered.  6. Discussion: Commonalities of AI-induced Field Change & Pending Issues over Governance Through application of the developed framework, three main takeaways that have implications for wider AI-induced field change and legitimization have been found. At a general level, these refer to (1) altered power-dependencies between humans and machines, (2) unresolved questions over data-use and control, as well as (3) issues with the current elaboration of institutional infrastructure that surrounds many AI applications. First, the autonomy of AI agents can affect existing power-dependencies, which may cause friction as human behavior and ways of organization are influenced in ways that are hard to identify ex-ante (Curchod et al., 2020). In examples such as facial recognition, judicial AI-systems, autonomous vehicles, and so on, the AI-agent gains determining power over human actors, which have to trust the identifications or predictions of the AI-agent. This transfer of autonomy is contingent on systemic trust, which is based on conceptualizations of technological maturity and ideas of machine-augmented perception that is expected to operate at cognitive levels that are equal to – or in many cases exceeds those of a human operator. Issues with field-level legitimization and nascent processes of institutionalization are therefore likely to arise when emerging systems are inaccurate, unsafe, or intransparent, which erode trust across applications and causes fields to stay fragmented and logics to grow incoherent. Analyzing the field trajectories of these cases involves assessing what it takes for altered power-dependencies to be conceived as legitimate practices, which is crucial for a field to move from fragmentation or contestation towards greater alignment of digital and institutional infrastructures. Second, an incentive for data-centralization is inherent in most digital infrastructures (based on technical and economic logics), which has implications for associated forms of organization. A lack of transparency during the processes of data collection, as well as in markets for data, are leaving large populations unaware of where and how their personal data and information is being used, stored, and traded, as well as for what purposes (Mittelstadt et al., 2016). The current organization of many digital infrastructures thus come with the risk of deteriorating public trust in digital institutional infrastructures if data-sources are used for socially disputed measures of public (e.g., safety) and private (e.g., market-based) forms of surveillance (Zuboff, 2019), or are being misused, e.g., due to large-scale data-breaches (Isaak & Hanna, 2018). This implies that the legitimacy of AI-agents is highly contingent on legitimate collection, use, and ownership of data, which otherwise could be a source of dispute that causes field-level disintegration. Regulations such as the European Union’s General Data Protection Regulation (GDPR) should be seen as the first step of elaborating institutional infrastructure that seeks to move fields engaged in data-collection from the contested quadrants towards greater establishment and coherency in logics. Over time this could imply a conceptual shift of companies moving from “owners” towards “custodians” of individuals’ private data. Opening access to data and developing interactivity, as well as an increased sense of ownership with users, is a step that could gain traction in order to smoothen existing information asymmetries between central actors and individual end-users (Tene & Polonetsky, 2013). Similarly, empowering users to better understand and perhaps interact with certain AI-agents (e.g., recommender engines) would empower these with a greater sense of ownership over how streams of information are utilized and handled, as well as impacting individual practices and forms of behaviorThird, where institutional infrastructure is considered inadequate during phases of market expansion, peripheral actors, such as civil society organizations, frequently work on outlining insufficient governance arrangements (Star, 2002). In many cases, it is important that institutional infrastructure is elaborated before negative externalities start to erode systemic and institutional levels of trust, which causes a field to grow fragmented. If trust is eroded past certain barriers, technology developers and adopters are likely to experience severe pushback from the general public. Public pushback forces central actors from the private sector to engage in new measures of self-regulation, which in some cases means scaling back digital infrastructure until a policy-vacuum is filled by new legislative provisions, such as in the case of facial recognition technologies. When logics are at odds with existing power structures or violate existing governance arrangements, these are also more likely to be resisted (Furnari, 2016). At the same time, institutional infrastructures must emerge as more adaptive forms of organization that are able to take into account the myriad ways in which modular AI-systems influence and shape existing practices, ways of behavior, and forms of organization. This warrants that new types of institutional engineering have to be embraced in order to keep up with rapidly expanding digital infrastructures while alleviating the pacing problem (Hagemann et al., 2018). Proposed measures of institutional adaptation to mitigate AI-induced externalities include enhanced measures of algorithmic auditing carried out by companies (Zarsky, 2016), third-party auditors (Clark & Hadfield, 2019), or external regulators (Tutt, 2016).Auditing can create an ex-post procedural record of complex algorithmic decision-making in order to track inaccurate decisions or to detect forms of discrimination, as well as biased data, practices, and other harms (Mittelstadt et al., 2016). When algorithms are designed without considering a population’s or community’s needs, it has become more apparent that both the algorithm and its implementer are likely to experience public pushback or outright rejection, which obstructs processes of AI legitimacy and adoption (Whittaker et al., 2018). As a growing number of fields continue to migrate from traditional forms of linear programming and further embrace autonomous learning algorithms – behavioral control is gradually transferred from the programmer to the algorithm and its operating environment (Matthias, 2004). During this process, “the modular design of systems can mean that no single person or group can fully grasp the manner in which the system will interact or respond to a complex flow of new inputs” (Allen, Wallach, & Smit, 2006: 14). In order to cope with AI-induced complexities, new governance structures have to be co-invented through greater stakeholder engagement among companies, civil society organizations, as well as policymakers in order to secure the inclusion of affected communities in the development of just algorithmic systems and processes going forward (Lee et al., 2019). The tradeoffs between algorithmic accuracy, transparency, and use of data, as well as the rights to privacy, explanation, and right of redress, remain subject to ongoing forms of mediation in relation to the concomitant organizational practices that emerge at the intersection of human-machine-based interactions. While these tradeoffs have wide-ranging implications for the kind of institutions that are likely to emerge, the devising of inclusive yet reflexive institutional infrastructures that are able to encompass a wide variety of AI-associated risks remains a crucial area to be studied for years to come.7. ConclusionThe increased presence of AI-agents embedded in varying forms of organization entails that a whole range of AI-induced institutions are currently emerging. This paper makes three contributions that help elicit the ways in which AI-induced fields are subject to varying degrees of legitimacy as well as processes of institutionalization.First, the paper develops a novel framework for analyzing AI-induced field change. Second, through an illustration of the framework and application of the concept of pathways of change, a set of common grounds for contestation associated with AI-induced field change have been found. These relate to altered power-dependencies between humans and machines, the importance of data as a source of legitimacy, and the need for more official rulemaking to guide the expansion of AI applications. At last, the paper points to the need for more adaptive organizations to emerge in response to AI-systems.The notion of pathways of change helps elicit the varying ways in which novel AI solutions are resisted, rejected, or accepted as legitimate practices over time. Assessing where a field is currently positioned, as well as what its potential trajectories are, or could be, and what needs to be done for a field to grow established and become broadly legitimatized over time, are essential considerations for stakeholders to take into account. Such deliberations contribute to secure greater alignment between digital and institutional infrastructures, which is important in terms of mitigating negative externalities going forward. The logics of any algorithmic interaction, as well as transparency with the information that guides the interaction, needs to be broadly examined in order to get a better understanding of how a given AI-agent affects and potentially alters existing dependencies between humans and machines, as well as between humans and new forms of organization. Only by understanding where certain negative externalities could potentially arise can organizations that are responsible for algorithmic development or implementation work on establishing the necessary institutional infrastructure (i.e., standards, rules, and processes) to keep such externalities in check. Thinking about where algorithmic decisions, including their basis for taking actions, as well as the rationales on which such actions are predicated, can help avoid fragmentation of logics, which causes practices and eventually fields to grow unstable.  Transparent and reliable AI systems, as well as enhanced human-AI interactions, is a key element for the trajectory of most AI fields on their road to secure a broad sense of social legitimacy as well as growing established over time. The dialectical relationship between stakeholders from private, public, and civil-society continues to play an important role in mediating disruptive AI technologies. It takes broad engagement, transparency as well as greater information-symmetry to build systemic trust across AI-systems and human-machine based interactions. As novel digital infrastructures continue to emerge, it is important that their road to becoming institutionalized structures of society is thoroughly vetted and mitigated in order to secure fair, equitable, and trustworthy socio-technical interactions in the years to come.Bibliography(PDPC), P. D. P. C. S., & (IMDA), I. M. D. A. (2020). Model Artificial Intelligence Governance Framework Second Edition.Allen, C., Wallach, W., & Smit, I. (2006). Why machine ethics? Machine Ethics, 51–61. https://doi.org/10.1017/CBO9780511978036.005Amis, J., Slack, T., & Hinings, C. R. (2004). The Pace, Sequence, and Linearity of Radical Change Author(s): The Academy of Management Journal, 47(1), 15–39.Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). Concrete Problems in AI Safety. ArXiv, 1–29. Retrieved from http://arxiv.org/abs/1606.06565Arnold, M., Piorkowski, D., Reimer, D., Richards, J., Tsay, J., Varshney, K. R., … Olteanu, A. (2019). FactSheets: Increasing trust in AI services through supplier’s declarations of conformity. IBM Journal of Research and Development, 63(4–5), 1–13. https://doi.org/10.1147/JRD.2019.2942288Baskerville, R. L., Myers, M. D., & Yoo, Y. (2019). Digital First: The Ontological Reversal and New Challenges for IS Research. MIS Quarterly.Beer, D. (2017). The social power of algorithms. Information Communication and Society, 20(1), 1–13. https://doi.org/10.1080/1369118X.2016.1216147Bender, E. M., & Friedman, B. (2018). Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics, 6, 587–604. https://doi.org/10.1162/tacl_a_00041Botzem, S., & Dobusch, L. (2012). Standardization Cycles?: A Process Perspective on the Formation and Diffusion of Transnational Standards. Organization Studies, 5–6(33), 737–762. https://doi.org/10.1177/0170840612443626Brynjolfsson, E., & McAfee, A. (2014). The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies. Quantitative Finance. https://doi.org/10.1080/14697688.2014.946440Castel, P., & Friedberg, E. (2010). Institutional change as an interactive process: The case of the modernization of the French cancer centers. Organization Science, 21(2), 311–330. https://doi.org/10.1287/orsc.1090.0442Clark, J., & Hadfield, G. K. (2019). REGULATORY MARKETS FOR AI SAFETY. Conference Paper at ICLR 2019.Constantinides, P., Henfridsson, O., & Parker, G. G. (2018). Platforms and infrastructures in the digital age. Information Systems Research, 29(2), 381–400. https://doi.org/10.1287/isre.2018.0794Coyle, D., Diepeveen, S., Wdowin, J., Kay, L., & Tennison, J. (2020). The Value of Data: Policy Implications. Bennett Institute for Public Policy, Cambridge in Partnership with the Open Data Institute.Curchod, C., Patriotta, G., Cohen, L., & Neysen, N. (2020). Working for an Algorithm: Power Asymmetries and Agency in Online Work Settings. Administrative Science Quarterly, 65(3), 644–676. https://doi.org/10.1177/0001839219867024Dacin, M. T., Ventresca, M. J., & Beal, B. D. (1999). The embeddedness of organizations: Dialogue & directions. Journal of Management, 25(3), 317–356. https://doi.org/10.1177/014920639902500304Davenport, T., & Kalakota, R. (2019). The potential for artificial intelligence in healthcare. Future Healthcare Journal, 6(2), 94–98.DiMaggio, P. J., & Powell, W. W. (1983). The Iron Cage Revisited?: Institutional Isomorphism and Collective Rationality in Organizational Fields. American Sociological Review, 48(2), 147–160.Djelic, M., & Sahlin-Andersson, K. (2006). Transnational governance: Institutional dynamics of regulation. Cambridge: Cambridge University Press.Fligstein, N. (2001). Social skill and the theory of fields. Sociological Theory, 19(2), 105–125. https://doi.org/10.1111/0735-2751.00132Fligstein, N., & McAdam, D. (2012). A theory of fields. Oxford, U.K: Oxford University Press.Floridi, L. (2014). The Fourth Revolution: How the infosphere is reshaping human reality. Oxford, U.K: Oxford University Press.Floridi, L., & Sanders, J. W. (2004). On the morality of artificial agents Luciano. Minds and Machines. https://doi.org/10.1023/BFurnari, S. (2014). Interstitial Spaces: Micro-Interaction Settings and the Genesis of New Practices between Institutional Fields. Academy of Management Review, 39(4), 1–55.Furnari, S. (2016). Institutional fields as linked arenas: Inter-field resource dependence, institutional work and institutional change. Human Relations, 69(3), 551–580. https://doi.org/10.1177/0018726715605555Garud, R. (2008). Conferences as venues for the configuration of emerging organizational fields: The case of cochlear implants. Journal of Management Studies, 45(6), 1061–1088. https://doi.org/10.1111/j.1467-6486.2008.00783.xGarud, R., Jain, S., & Kumaraswamy, A. (2002). Institutional entrepreneurship in the sponsorship of common technological standards: The case of Sun Microsystems and Java. Academy of Management Journal, 45(1), 196–214. https://doi.org/10.2307/3069292Garud, R., & Karnøe, P. (2003). Bricolage versus breakthrough: distributed and embedded agency in technology entrepreneurship. Research Policy, 32, 277–300.Gawer, A., & Phillips, N. (2013). Institutional Work as Logics Shift?: The Case of Intel ’ s Transformation to Platform Leader. Organization Science, 34(8), 1035 –1071. https://doi.org/10.1177/0170840613492071Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallanch, H., Daume, H., & Crawford, K. (2020). Datasheets for Datasets. Arxiv.Ghaffary, S., & Molla, R. (2019, December 10). Here’s where the US government is using facial recognition technology to surveil Americans. Vox. Retrieved from https://www.vox.com/recode/2019/7/18/20698307/facial-recognition-technology-us-government-fight-for- the-futureGreenwood, R., Raynard, M., Kodeih, F., Micelotta, E. R., & Lounsbury, M. (2011). Institutional complexity and organizational responses. Academy of Management Annals, 5(1), 317–371. https://doi.org/10.1080/19416520.2011.590299Greenwood, R., Suddaby, R., & Hinings, C. R. (2002). Theorizing change: The role of professional associations in the transformation of institutionalized fields. Academy of Management Journal, 45(1), 58–80. https://doi.org/10.2307/3069285Grigorescu, S., Trasnea, B., Cocias, T., & Macesanu, G. (2020). A survey of deep learning techniques for autonomous driving. Journal of Field Robotics, 37(3), 362–386. https://doi.org/10.1002/rob.21918Hagemann, R., Huddleston, J., & Thierer, A. D. (2018). Soft Law for Hard Problems: The Governance of Emerging Technologies in an Uncertain Future. Colorado Technology Law Journal (Vol. 17). Retrieved from https://heinonline.org/HOL/Page?handle=hein.journals/jtelhtel17&id=52&div=8&collection=journals%0Ahttps://ssrn.com/abstract=3118539Hao, K. (2020a). IBM’s photo-scraping scandal shows what a weird bubble AI researchers live in. MIT Technology Review. Retrieved from https://www.technologyreview.com/2019/03/15/136593/ibms-photo-scraping-scandal-shows-what-a-weird-bubble-ai-researchers-live-in/Hao, K. (2020b). The two-year fight to stop Amazon from selling face recognition to the police. MIT Technology Review.Henfridsson, O., Nandhakumar, J., Scarbrough, H., & Panourgias, N. (2018). Recombination in the open-ended value landscape of digital innovation. Information and Organization, 28(2), 89–100. https://doi.org/10.1016/j.infoandorg.2018.03.001Hengstler, M., Enkel, E., & Duelli, S. (2016). Applied artificial intelligence and trust-The case of autonomous vehicles and medical assistance devices. Technological Forecasting and Social Change, 105, 105–120. https://doi.org/10.1016/j.techfore.2015.12.014Hensmans, M. (2003). Social movement organizations: A metaphor for strategic actors in institutional fields. Organization Studies, 24(3), 355–381. https://doi.org/10.1177/0170840603024003908Hill, K. (2020a). The Secretive Company That Might End Privacy as We Know It. New York Times. Retrieved from https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.htmlHill, K. (2020b). Wrongfully Accused by an Algorithm. New York Times. Retrieved from https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.htmlHinings, B., Gegenhuber, T., & Greenwood, R. (2018). Digital innovation and transformation: An institutional perspective. Information and Organization, 28(1), 52–61. https://doi.org/10.1016/j.infoandorg.2018.02.004Hinings, B., Logue, D., & Zietsma, C. (2017). Fields, Institutional Infrastructure and Governance. In The SAGE Handbook of Organizational Institutionalism (pp. 163–189).Holm, P. (1995). The Dynamics of Institutionalization: Transformation Processes in Norwegian Fisheries. Administrative Science Quarterly, 40(3), 398–422.Hosanagar, K., Fleder, D., Lee, D., & Buja, A. (2014). Will the global village fracture into tribes recommender systems and their effects on consumer fragmentation. Management Science, 60(4), 805–823. https://doi.org/10.1287/mnsc.2013.1808Isaak, J., & Hanna, M. J. (2018). User Data Privacy: Facebook, Cambridge Analytica, and Privacy Protection. Computer, 51(8), 56–59. https://doi.org/10.1109/MC.2018.3191268Kallinikos, J. (2011). Governing through Technology: Information Artefacts and Social Practices. New York: Palgrave Macmillan. New York: Palgrave Macmillan.Kiran, A. H., Oudshoorn, N., & Verbeek, P. P. (2015). Beyond checklists: toward an ethical-constructive technology assessment. Journal of Responsible Innovation, 2(1), 5–19. https://doi.org/10.1080/23299460.2014.992769Krishna, A. (2020). IBM CEO’s Letter to Congress on Racial Justice Reform. Retrieved from https://www.ibm.com/blogs/policy/facial-recognition-sunset-racial-justice-reforms/Lash, S. (2007). Power after Hegemony: Cultural Studies in Mutation? Theory, Culture & Society, 24(3), 55–78. https://doi.org/10.1177/0263276407075956Lawrence, T. B., & Suddaby, R. (2006). Institutions and institutional work. In The SAGE Handbook of Organization Studies (pp. 215–254). https://doi.org/10.4135/9781848608030.n7Lee, M. K. (2018). Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. Big Data and Society, 5(1), 1–16. https://doi.org/10.1177/2053951718756684Lee, M. K., Kusbit, D., Kahng, A., Kim, J. T., Yuan, X., Chan, A., … Procaccia, A. D. (2019). Webuildai: Participatory framework for algorithmic governance. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW). https://doi.org/10.1145/3359283Mackenzie, A. (2006). Cutting Code: Software and Sociality. New York: Peter Lang Publishing.Macnish, K. (2012). Unblinking eyes: The ethics of automating surveillance. Ethics and Information Technology, 14(2), 151–167. https://doi.org/10.1007/s10676-012-9291-0Maguire, S., & Hardy, C. (2009). Discourse and Deinstitutionalization: The Decline of DDT. The Academy of Management Journal, 52(1), 148–178. Retrieved from https://doi.org/10.5465/amj.2009.36461993Marchant, G. E., & Lindor, R. A. (2012). The Coming Collision Between Autonomous Vehicles and the Liability System THE COMING COLLISION BETWEEN AUTONOMOUS VEHICLES AND THE LIABILITY SYSTEM. Santa Clara Law Review Article, 52(4).Markus, M. L. (2017). Datification, Organizational Strategy, and IS Research: What’s the Score? Journal of Strategic Information Systems, 26(3), 233–241. https://doi.org/10.1016/j.jsis.2017.08.003Matthias, A. (2004). The responsibility gap: Ascribing responsibility for the actions of learning automata. Ethics and Information Technology, 6(3), 175–183. https://doi.org/10.1007/s10676-004-3422-1Mckay, C. (2020). Predicting risk in criminal procedure: actuarial tools, algorithms, AI and judicial decision-making. Current Issues in Criminal Justice, 32(1), 22–39. https://doi.org/10.1080/10345329.2019.1658694Meyer, J. W., & Rowan, B. (1977). Institutionalized Organizations?: Formal Structure as Myth and Ceremony. American Journal of Sociology, 83(2), 340–363.Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., … Gebru, T. (2019). Model cards for model reporting. FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency, (Figure 2), 220–229. https://doi.org/10.1145/3287560.3287596Mittelstadt, B. D., Allo, P., Taddeo, M., Wachter, S., & Floridi, L. (2016). The ethics of algorithms: Mapping the debate. Big Data and Society, (December), 1–21. https://doi.org/10.1177/2053951716679679Möller, J., Trilling, D., Helberger, N., & van Es, B. (2018). Do not blame it on the algorithm: an empirical assessment of multiple recommender systems and their impact on content diversity. Information Communication and Society, 21(7), 959–977. https://doi.org/10.1080/1369118X.2018.1444076Munir, K. A., & Phillips, N. (2005). The birth of the “Kodak moment”: Institutional entrepreneurship and the adoption of new technologies. Organization Studies, 26(11), 1665–1687. https://doi.org/10.1177/0170840605056395Nambisan, S. (2016). Digital Entrepreneurship: Toward a Digital Technology Perspective of Entrepreneurship. Entrepreneurship: Theory and Practice, 41(6), 1029–1055. https://doi.org/10.1111/etap.12254National Institute of Standards and Technology (NIST). (2020). NIST. Retrieved from https://pages.nist.gov/frvt/html/frvt11.html#overviewhttps://www.technologyreview.com/2019/03/15/13 6593/ibms-photo-scraping-scandal-shows-what-a-weird-bubble-ai-researchers-live-in/Newell, S., & Marabelli, M. (2015). Strategic opportunities (and challenges) of algorithmic decision-making: A call for action on the long-term societal effects of “datification.” Journal of Strategic Information Systems, 24(1), 3–14. https://doi.org/10.1016/j.jsis.2015.02.001North, D. C. (1990). Institutions, Institutional Change and Economic Performance. Institutions, Institutional Change and Economic Performance. North, Douglass C. https://doi.org/10.1017/CBO9780511606892.012Nuno, N., Gomes, A. de, & Kontschieder, V. (2021). AI Impact Assessment: A Policy Prototyping Experiment, (January).Orlikowski, W. J. (2007). Sociomaterial practices: Exploring technology at work. Organization Studies, 28(9), 1435–1448. https://doi.org/10.1177/0170840607081138Orlikowski, W. J., & Scott, S. V. (2008). 10 Sociomateriality: Challenging the Separation of Technology, Work and Organization. The Academy of Management Annals, 2(1), 433–474. https://doi.org/10.1080/19416520802211644Parasuraman, R., & Manzey, D. H. (2010). Complacency and bias in human use of automation: An attentional integration. Human Factors, 52(3), 381–410. https://doi.org/10.1177/0018720810376055Patvardhan, S. D., Gioia, D. A., & Hamilton, A. L. (2015). Weathering a meta-level identity crisis: Forging a coherent collective identity for an emerging field. Academy of Management Journal, 58(2), 405–435. https://doi.org/10.5465/amj.2012.1049Pisano, G. P., & Teece, D. J. (2007). How to Capture Value from Innovation: Shaping Intellectual Property and Industry Architecture. California Management Review.Powell, W. W., Oberg, A., Korff, V., Oelberger, C., & Kloos, K. (2017). Institutional analysis in a digital era: Mechanisms and methods to understand emerging fields. In New Themes in Institutional Analysis: Topics and Issues from European Research (pp. 305–344). https://doi.org/10.4337/9781784716875.00016Powell, W. W., & Sandholtz, K. W. (2012). Amphibious entrepreneurs and the emergence of organizational forms. Strategic Entrepreneurship Journal, 6(2), 94–115. https://doi.org/10.1002/sej.1129Rai, A., Constantinides, P., & Sarker, S. (2019). Editor’s comments: Next-Generation Digital Platforms: Toward Human–AI Hybrids. MIS Quarterly, 43(1).Rao, H., Morrill, C., & Zald, M. N. (2000). Power plays: How social movements and collective action create new organizational forms. Research in Organizational Behavior, 22(June), 237–281. https://doi.org/10.1016/s0191-3085(00)22007-8Raynard, M. (2016). Deconstructing complexity: Configurations of institutional complexity and structural hybridity. Strategic Organization, 14(4), 310–335. https://doi.org/10.1177/1476127016634639Reay, T., & Hinings, C. R. (2005). The recomposition of an organizational field: Health care in Alberta. Organization Studies, 26(3), 351–384. https://doi.org/10.1177/0170840605050872Reay, T., & Hinings, C. R. (2009). Managing the rivalry of competing institutional logics. Organization Studies, 30(6), 629–652. https://doi.org/10.1177/0170840609104803Reisman, D., Schultz, J., Crawford, K., & Whittaker, M. (2018). Algorithmic impact assessments: A practical framework for public agency accountability. AI Now Institute, (April), 22. Retrieved from https://ainowinstitute.org/aiareport2018.pdfRussell, S. J., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach. Artificial Intelligence. Prentice-Hall. https://doi.org/10.1017/S0269888900007724Schlesinger, A., O’Hara, K. P., & Taylor, A. S. (2018). Let’s talk about race: Identity, chatbots, and AI. Conference on Human Factors in Computing Systems - Proceedings, 2018-April, 1–14. https://doi.org/10.1145/3173574.3173889Scott, R. W. (2014). Institutions and Organizations, 4th ed.Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & Vertesi, J. (2019). Fairness and abstraction in socio-technical systems. In FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency (pp. 59–68). https://doi.org/10.1145/3287560.3287598Star, S. L. (2002). Infrastructure and ethnographic practice: Working on the fringes. Scandinavian Journal of Information Systems, 14(2), 107–122.Stoyanovich, J., & Howe, B. (2019). Nutritional Labels for Data and Models. Data Engineering, (1926250), 13.Sunstein, C. R. (2007). Republic.com 2.0. Princeton, NJ: Princeton University Press.Tene, O., & Polonetsky, J. (2013). Big Data for All?: Privacy and User Control in the Age of Analytics Big Data for All?: Privacy and User Control in the. Northwestern Journal of Technology and Intellectual Property, 11(5).Thornton, P. H., & Ocasio, W. (1999). Institutional logics and the historical contingency of power in organizations: Executive succession in the higher education publishing industry, 1958-1990. American Journal of Sociology, 105(3), 801–843. https://doi.org/10.1086/210361Thornton, P. H., Ocasio, W., & Lounsbury, M. (2012). Introduction to the Institutional Logics Perspective. In The Institutional Logics Perspective: A New Approach to Culture, Structure and Process. https://doi.org/10.1093/acprofTilson, D., Lyytinen, K., & Sørensen, C. (2010). Digital infrastructures: The missing IS research agenda. Information Systems Research, 21(4), 748–759. https://doi.org/10.1287/isre.1100.0318Timmermans, S., & Epstein, S. (2010). A world of standards but not a standard world: Toward a sociology of standards and standardization. Annual Review of Sociology, 36, 69–89. https://doi.org/10.1146/annurev.soc.012809.102629Tutt, A. (2016). AN FDA FOR ALGORITHMS. Administrative Law Review, 69(1).Van Alstyne, M., & Brynjolfsson, E. (2005). Global village or cyber-balkans? Modeling and measuring the integration of electronic communities. Management Science, 51(6), 851–868. https://doi.org/10.1287/mnsc.1050.0363Varshney, K. R., & Alemzadeh, H. (2017). On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products. Big Data, 5(3), 246–255. https://doi.org/10.1089/big.2016.0051Waddock, S. (2008). Building a new institutional infrastructure for corporate responsibility. Academy of Management Perspectives, 22(3), 87–108. https://doi.org/10.5465/AMP.2008.34587997Whittaker, M., Crawford, K., Dobbe, R., Fried, G., Kaziunas, E., Mathur, V., … Schwartz, O. (2018). AI Now 2018 Report. AI Now, (December), 1–62. Retrieved from https://ainowinstitute.org/AI_Now_2018_Report.pdfYang, K., Stoyanovich, J., Asudeh, A., Howe, B., Jagadish, H. V., & Miklau, G. (2018). A nutritional label for rankings. In Proceedings of the ACM SIGMOD International Conference on Management of Data (pp. 1773–1776). https://doi.org/10.1145/3183713.3193568Yoo, Y., Henfridsson, O., & Lyytinen, K. (2010). The new organizing logic of digital innovation: An agenda for information systems research. Information Systems Research, 21(4), 724–735. https://doi.org/10.1287/isre.1100.0322Zarsky, T. (2016). The Trouble with Algorithmic Decisions: An Analytic Road Map to Examine Efficiency and Fairness in Automated and Opaque Decision Making. Science, Technology, & Human Values, 41(1), 118–132. https://doi.org/10.1177/0162243915605575Zhu, H., Yu, B., Halfaker, A., & Terveen, L. (2018). Value-Sensitive Algorithm Design. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW), 1–23. https://doi.org/10.1145/3274463Zietsma, C., Groenewegen, P., Logue, D., & Hinings, C. R. (Bob). (2017). FIELD OR FIELDS? BUILDING THE SCAFFOLDING FOR CUMULATION OF RESEARCH ON INSTITUTIONAL FIELDS. Academy of Management Annals, 11(1). https://doi.org/10.1017/CBO9781107415324.004Zietsma, C., & Lawrence, T. B. (2010). Institutional work in the transformation of an organizational field: The interplay of boundary work and practice work. Administrative Science Quarterly, 55(2), 189–221. https://doi.org/10.2189/asqu.2010.55.2.189Zilber, T. B. (2007). Stories and the discursive dynamics of institutional entrepreneurship: The case of Israeli high-tech after the bubble. Organization Studies, 28(7), 1035–1054. https://doi.org/10.1177/0170840607078113Zuboff, S. (1988). In the age of the smart machine: the future of work and power. New York: Basic Books.Zuboff, S. (2019). The Age of Surveillance Capitalism. New York: PublicAffairs.11